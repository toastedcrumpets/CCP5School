<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>CCP5 Summer School, Monte Carlo</title>
    <meta name="description" content="A presentation by Marcus Bannerman.">
    <meta name="author" content="Marcus Bannerman <m.bannerman@gmail.com">
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
    <meta name="viewport" content="initial-scale=1">
	<style>
		loaded {color:aquamarine}
	</style>
    <link rel="stylesheet" href="reveal.js/dist/reveal.css"></link>
    <link rel="stylesheet" href="Font-Awesome/css/font-awesome.min.css"></link>
    <!-- Default themes -->
    <!-- <link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme"></link>-->
    <!-- For syntax highlighting -->
    <link rel="stylesheet" href="reveal.js/dist/theme/night.css"></link>
    <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM" crossorigin="anonymous"></script>
    <script
      src="https://code.jquery.com/jquery-3.6.0.min.js"
      integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4="
      crossorigin="anonymous"></script>
    <script>
     /**
      * Simple object check.
      * @param item
      * @returns {boolean}
      */
     function isObject(item) {
       return (item && typeof item === 'object' && !Array.isArray(item));
     }

     /**
      * Deep merge two objects.
      * @param target
      * @param ...sources
      */
     function mergeDeep(target, ...sources) {
       if (!sources.length) return target;
       const source = sources.shift();

       if (isObject(target) && isObject(source)) {
	 for (const key in source) {
	   if (isObject(source[key])) {
             if (!target[key]) Object.assign(target, { [key]: {} });
             mergeDeep(target[key], source[key]);
	   } else {
             Object.assign(target, { [key]: source[key] });
	   }
	 }
       }

       return mergeDeep(target, ...sources);
     }
     
     var default_plot_Layout = {
       autosize:true,
       font: {
	 color: 'white',
	 size: 15,
       },
       showlegend:true,
       legend: {
	 x:1, y:1, xanchor: 'right',
       },
       xaxis: {
	 gridcolor:'white',
       },
       yaxis: {
	 automargin:true,
	 gridcolor:'white',
       },
       plot_bgcolor: '#000',
       paper_bgcolor: '#000E',
     };

     var default_plot_Config = {
       responsive: false,
       displaylogo: false,
     };
    </script>
    <script src="reveal.js/dist/reveal.js"></script>
    <script src="reveal.js/plugin/math/math.js"></script>
    <script src="reveal.js/plugin/markdown/markdown.js"></script>
    <script src="reveal.js/plugin/highlight/highlight.js"></script>
    <script src="reveal.js/plugin/notes/notes.js"></script>
    <script src="codeButtons.js"></script>
    <link rel="stylesheet" href="UoA.css" id="theme"></link>    
  </head>
  <body>
    <div class="reveal">
      <div class="slides" style="border: 1px solid darkslategray">
	<section>
	  <section data-background-image="img/UoAbackground.jpg">
	    <div class="backbox">
	      <h3>Notes on Molecular Simulation</h3>
	      <h3>CCP5 Summer School</h3>
	    </div>
	    <p>
	      Marcus N. Bannerman<br/>
	      <a href="mailto:m.campbellbannerman@abdn.ac.uk">m.campbellbannerman@abdn.ac.uk</a>
	      <object type="image/svg+xml" data="img/UoALarge_reversed.svg" width="30%"  style="margin:2em auto;display:block;">
		Your browser does not support SVG
	      </object>
	      Access the slides here:
	      <a href="http://www.marcusbannerman.co.uk/CCP5School">www.marcusbannerman.co.uk/CCP5School</a>
	    </p>
	  </section>
	  <section>
	    <h3 style="margin-top:1em;">Current Lectures</h3>
	    <p>
	      <ul>
		<li>
		  <a href="https://colab.research.google.com/github/toastedcrumpets/CCP5_Python_examples/blob/master/Table_of_contents_for_collab_research_google_com.ipynb">Python pre-school notes (hosted on Google collab)</a>
		</li>
		<li>
		  <a href="#SM1">Statistical mechanics (1 & 2)</a>
		</li>
		</ul>
		<h3>My old notes (ignore)</h3>
		<ul>
		<li style="font-size:75%;">
		  <a href="#MC1">Monte carlo 1</a>
		</li>
		<li style="font-size:75%;">
		  <a href="#MC2">Monte carlo 2</a>
		</li>
		<li style="font-size:75%;">
		  <a href="#MC3">Monte carlo 3</a>
		</li>
	      </ul>
	    </p>
	  </section>
	</section>
	<section id="SM1">
	  <section data-background="img/Boltzmann_grave.jpg">
	    <h2 class="backbox">Statistical mechanics</h2>
	  </section>	    
	  <section data-background="img/Boltzmann_grave.jpg">
	    <p class="backbox"><em>
	      Ludwig Boltzmann, who spent much of his life studying statistical mechanics, died in 1906, by his own hand. Paul Ehrenfest, carrying on the work, died similarly in 1933. Now it is our turn to study statistical mechanics. Perhaps it will be wise to approach the subject cautiously.
	    </em>
	    </p>
	    <p class="backbox" style="float:right">(Opening lines of "States of Matter", by D.L. Goodstein).</p>
	    <div class="fragment backbox" style="font-size:75%;clear:both;">
	      <p>
		Other excellent stat-mech texts include:
	      </p>
	      <ul>
		<li><a href="https://www.researchgate.net/publication/27693700_Statistical_mechanics_for_computer_simulators">&ldquo;Statistical mechanics for computer simulators,&rdquo;</a> Daan Frenkel (free/online/40 pages)</li>
		<li>&ldquo;Physical chemistry,&rdquo; P. W. Atkins.</li>
		<li>&ldquo;Molecular driving forces,&rdquo; K. A. Dill and S. Bromberg</li>
		<li>&ldquo;Statistical mechanics: A survival guide,&rdquo; M. Glazer and J. Wark</li>
		<li>&ldquo;Statistical mechanics,&rdquo; D. A. McQuarrie</li>
		<li>&ldquo;Statistical mechanics, principles and selected applications&rdquo; T. L. Hill</li>
		<li>&ldquo;Introduction to modern statistical mechanics&rdquo; David Chandler</li>
		<li>&ldquo;Statistical mechanics: Theory and molecular simulation&rdquo; Mark Tuckerman</li>
		<li>&ldquo;Computer simulation of liquids&rdquo; M. P. Allen and D. J. Tildesley</li>
	      </ul>
	    </div>
	  </section>
	  <section>
	    <p>
	      Two hours/lectures is nothing, I will hopelessly fail to
	      teach you it all, so I'll give you the key touch points,
	      and try my best to be different/interesting/intuitive.
	    </p>
	    <p class="fragment">
	      Interrupt me, question me, tell me how to do it better! Anything is better than silence.
	    </p>
		<p class="fragment">
			I'll put &ldquo;loaded&rdquo; words which I think could be
			interesting to ask about <loaded>in this colour</loaded> to remind
			me to tell you about them, and to remind you to ask!
		</p>
	  </section>
	  <section>
	    <p>
	      Statistical mechanics is our bridge from the microscopic
	      scale (simulating individual atoms/molecules) to the
	      &ldquo;macroscopic&rdquo; measurements we make in
	      experiments (i.e., density, temperature, heat capacity, pressure,
	      <loaded>free-energy</loaded>).
		</p>
		<p>
			<img src="img/DiffusionMicroMacro.gif" style="width:45%;display:block;margin: 0 auto;"/>
		</p>
		<p>
				We will use Statistical Mechanics to connect to <b>thermodynamics</b>, and that
				is the focus here (statistical
				thermodynamics/equilibrium statistical mechanics), but
				it also is used to explore dynamics.
		</p>
	</section>
	</section>
	<section>
	  <section>
	    <h3>Thermodynamics Recap</h3>
	    <p>
	      Thermodynamics was/is a <loaded>phenomenological theory</loaded>,
	      based on observations of <b>state</b> variables,
	      $p,\,T,\,V,\,U,\,M,\,S,\ldots$ that <loaded>seem</loaded> to fully describe a
	      thermodynamic <b>system</b>.
	    </p>
	    <p class="fragment">
	      The power of thermodynamics arises from its ability to
	      find simple universal relationships between <loaded>observable</loaded>
	      state variables.</p>
	    <p class="fragment">
	      First law: <b>Energy is conserved</b>
	      \begin{align*}
	      U_{system}+U_{surroundings}={\rm Constant}
	      \end{align*}
	      which also implies the differential relationship
	      \begin{align*}
	      {\rm d}U_{system}=-{\rm d}U_{surroundings}
	      \end{align*}
	    </p>
		<p>
			<loaded>Have you ever seen energy?</loaded>
		</p>
	  </section>
	  <section>
	    <p>
	      Energy is conserved but can be transferred in many ways; however, heat
	      is <loaded>special</loaded> thanks to the second law so we distinguish
	      it from other transfers called work: \begin{align*} {\rm d}U_{system}
	      = \partial Q - \partial W \end{align*}
	    </p>
	    <p class="fragment">
	      Second law: Entropy must increase ${\rm d}S_{universe}>0$ but the key relationship is actually ${\rm
	      d}S > \partial Q/T$.
	    </p>
	    <p class="fragment">
	      This becomes ${\rm d}S=\partial Q/T$ if the process is
	      <loaded>reversible</loaded> (and we can make things almost reversible if
	      we chop them up enough so that the chunks/sub-systems are almost homogeneous), leading to

	      \begin{align*}
	      {\rm d}U = T\,{\rm d}S - \partial W
	      \end{align*}

	      for a reversible processes.
	    </p>
	  </section>
	  <section>
		<p>We have now made the</p>
	    <h3>Fundamental thermodynamic equation</h3>
	    <p>
	      \begin{align*}
	      {\rm d}U &= T\,{\rm d}S - \partial W
	      \\
	      &= T\,{\rm d}S - \sum_i {\color{red}F_i}\,{\color{teal}{\rm d}L_i}
	      \end{align*}
		  It relates its <loaded>natural</loaded> variables $U$, $S$, and $L_i$ together, where  
	      $L_i$ is built up by adding the work terms we can <loaded>observe/control</loaded> in our system.
	    </p>
	    <table>
	      <tbody>
		<tr>
		  <td style="border-right:4px solid white;">${\color{red}F_i}$</td>
		  <td>$p$</td>
		  <td>$\mu_\alpha$</td>
		  <td>$\gamma$</td>
		  <td>$\tau$</td>
		  <td>&hellip;</td>
		</tr>
		<tr>
		  <td style="border-right:4px solid white;">${\color{teal}L_i}$</td>
		  <td>$V$</td>
		  <td>$-N_\alpha$</td>
		  <td>$\Sigma$</td>
		  <td>$\dot{\omega}$</td>
		  <td>&hellip;</td>
		</tr>
	      </tbody>
	    </table>
		<p>
			For now, assume we want to calculate $U$ when controlling $S$, and
			$L_i$, but any rearrangement of this is possible (see implicit function theorem).
		</p>
	  </section>
	</section>
	<section>
	  <h3>Example</h3>
	  <img src="img/balloon.svg" style="width:45%"/>
	  <p>Consider we have some gas (system B), then we always have entropy $S$
		  in the fundamental equation, but also have a volume/pressure work as the gas can expand, as well
		  as mass/chemical-potential work as it flows through the boundary.</p>
	  <p>
	    If we suddenly decided the gas was inside a balloon, energy is
	    stored in the force/tension, $\gamma$, of the stretched elastic
	    surface, $\Sigma$.</p>
	  <p>
		If we tied the baloon and neglected diffusion through the skin, we can
		ignore the mass change (${\rm d}N=0$) and throw away this term!
	  </p>
	</section>
	<section>
		<h3>Example</h3>
		<img src="img/balloon.svg" style="width:45%"/>
		<p>
		  The <loaded>independent</loaded> variables $S$ and $V$ are unwieldy as
		  we can't measure entropy and the balloon's volume will change in
		  response to fluctuations in $T$ and $p$, so we need to change them,
		  but first lets solve the fundamental equation as this can be done in
		  its current form.</p>
	  </section>
	  <section>
	  <h3>Fundamental thermodynamic equation</h3>
	  <p>
	    \begin{align*}
	    {\rm d}U &= T\,{\rm d}S - \partial W
	    = T\,{\rm d}S - \sum_i F_i\,{\rm d}L_i
	    \end{align*}
	  </p>
	  <table>
	    <tbody>
	      <tr>
		<td style="border-right:4px solid white;">$F_i$</td>
		<td>$p$</td>
		<td>$\mu_\alpha$</td>
		<td>$\gamma$</td>
		<td>&hellip;</td>
	      </tr>
	      <tr>
		<td style="border-right:4px solid white;">$L_i$</td>
		<td>$V$</td>
		<td>$-N_\alpha$</td>
		<td>$\Sigma$</td>
		<td>&hellip;</td>
	      </tr>
	    </tbody>
	  </table>
	  <p>
	    Amazingly, as each term is a <b>conjugate</b> pairing of an <b>intensive</b> term and a differential <b>extensive</b> term,
	    this equation can be solved using Euler's
	    solution for homogeneous functions.

	    \begin{align*}
	    U(S,\,V,\,\left\{N_\alpha\right\},\ldots) &= T\,S -p\,V+\sum_\alpha \mu_\alpha\,N_\alpha + \ldots\\
	    \end{align*}
	  </p>
	</section>
	</section>
	<section>
	  <h3>Generating identities</h3>
	  <p>
	    <span style="font-size:70%">
	      \begin{align*}
	      U(S,\,V,\,\left\{N_\alpha\right\},\ldots) &= T\,S -p\,V+\sum_\alpha \mu_\alpha N_\alpha + \ldots
	      \\
	      {\rm d}U &= T\,{\rm d}S  -p\,{\rm d}V+\sum_\alpha \mu_\alpha\,{\rm d}N_\alpha + \ldots
	      \end{align*}
	    </span>
	    Comparing the equations we see lots of differential identities which
	    will later help us connect thermodynmics to microscopics as we'll have the derivatives:
	    <span style="font-size:70%">
	      \begin{align*}
	      \left(\frac{{\rm d}U}{{\rm d}S}\right)_{V,\,\left\{N_\alpha\right\},\ldots} &= T & \left(\frac{{\rm d}U}{{\rm d}V}\right)_{S,\,\left\{N_\alpha\right\},\ldots} &= -p
	      \\
	      \left(\frac{{\rm d}U}{{\rm d}N_\alpha}\right)_{S,\,V,\,\left\{N_\beta\neq\alpha\right\},\ldots} &= \mu & \ldots &= \ldots
	      \end{align*}
	    </span>
	  </p>
	  <div class="fragment">
	    <p>
	      In general, when we have a relationship between $U,\,S,\,V,\ldots$,
	      then it is a <b>generating function</b> for the rest of the
	      thermodynamic properties.
	    </p>
	    <div class="footnote">
	      Note that $U(S,\,V,\,\left\{N_\alpha\right\})$ can be rearranged to $S(U,\,V,\,\left\{N_\alpha\right\})$ and $\left(\frac{{\rm d}U}{{\rm d}S}\right)_X = \left(\frac{{\rm d}S}{{\rm d}U}\right)_X^{-1}$ so entropy is a generating function as well as energy.
	    </div>
	  </div>
	</section>
	<section>
	  <h3>Legendre Transformations</h3>
	  <p>
	    <span style="font-size:75%">
	      \begin{align*}
	      U(S,\,V,\,\left\{N_\alpha\right\},\ldots) &= T\,S -p\,V+\sum_\alpha \mu_\alpha N_\alpha + \ldots
	      \\
	      {\rm d}U &= T\,{\rm d}S  -p\,{\rm d}V+\sum_\alpha \mu_\alpha\,{\rm d}N_\alpha + \ldots
	      \end{align*}
	    </span>
	    $U,\,S,\,V$ can be challenging variables to measure/control, so we want
	    something more observable/controllable. The 
	    <b>conjugate</b> variables  $T,\,T^{-1},\,p$ are nicer for experiments (maybe not simulation though). 
	  </p>
	  <p>
	    A Legendre transformation defines a new <b>potential</b> that swaps the pair
	    around as <b>independent</b> variable. I.e. Defining the enthalpy $H = U + p\,V$ 
	    leads to:
	    <span style="font-size:75%">
	      \begin{align*}
	      {\rm d}H &= {\rm d}U + V\,{\rm d}p +p\,{\rm d}V\\
	      &= T\,{\rm d}S +V{\rm d}p+\sum_\alpha \mu_\alpha\,{\rm d}N_\alpha + \ldots
	      \\
	      H\left(S,\,p,\,\left\{N_\alpha\right\},\ldots\right) &= \ldots
	      \end{align*}
	    </span>
	  </p>
	</section>
	<section>
	  <h3>The thermodynamic table</h3>
	  <p style="font-size:65%">
	    \begin{align*}
	    U\left(S,\,V,\,\left\{N_\alpha\right\}\right)&= T\,S - p\,V + \sum_\alpha \mu_\alpha\,N_\alpha
	    &
	    {\rm d}U &= T\,{\rm d}S - p\,{\rm d}V + \sum_\alpha\mu_{\alpha}\,{\rm d} N_{\alpha}
	    \\
	    S\left(U,\,V,\,\left\{N_\alpha\right\}\right)&= \frac{U}{T} + \frac{p}{T}\,V - \sum_\alpha \frac{\mu_\alpha}{T}\,N_\alpha
	    &
	    {\rm d}S &= T^{-1}\,{\rm d}S + \frac{p}{T}{\rm d}V - \sum_\alpha\frac{\mu_{\alpha}}{T}{\rm d} N_{\alpha}
	    \end{align*}
	  </p>
	  <p style="font-size:65%">
	    <b>Legendre transforms</b>
	    \begin{align*}
	    H\left(S,\,p,\,\left\{N_\alpha\right\}\right)&= U {\color{red} + p\,V} = T\,S + \sum_\alpha\mu_\alpha\,N_\alpha
	    &
	    {\rm d}H &=
	    T\,{\rm d}S + V\,{\rm d}p + \sum_\alpha\mu_\alpha\,{\rm d} N_\alpha
	    \\
	    A\left(T,\,V,\,\left\{N_\alpha\right\}\right)&= U {\color{red}- T\,S} = - p\,V + \sum_\alpha\mu_\alpha\,N_\alpha
	    &
	    {\rm d}A &= -S\,{\rm d}T - p\,{\rm d}V +
	    \sum_\alpha\mu_{\alpha}\,{\rm d} N_{\alpha}
	    \\
	    G\left(T,\,p,\,\left\{N_\alpha\right\}\right)&= H {\color{red}- T\,S} = \sum_\alpha\mu_\alpha\,N_\alpha
	    &
	    {\rm d}G &= -S\,{\rm d}T + V\,{\rm d}p + \sum_\alpha\mu_\alpha\,{\rm
	    d} N_\alpha\\
		\ldots & & & \ldots
	    \end{align*}
	  </p>
	  <p>
		Every time we add a new work term, there is a new Legendre transform,
		and thus a new free energy (homework, find a new work term and name the
		free energy after yourself). 
	  </p>
	</section>
	<section>
	  <section>
	    <h3>Statistical mechanics by example</h3>
	    <p>
	      We've now covered all of thermodynamics (!). We're ready for statistical mechanics now.
	    </p>
	    <p class="fragment">
	      The simplest example I can think of that fits statistical
	      mechanics AND thermodynamics is the game of craps (the
	      sum of two dice).</p>
	    <p class="fragment">
	      We'll study this, and try to draw parallels to molecular
	      systems. Whenever the link isn't clear, let me know!
	    </p>
	    <p class="fragment">
	      Amazingly to me, we derive entropy first, then add
	      conservation of energy to bring in temperature, then the
	      rest of thermo follows.
	    </p>
	  </section>
	</section>
	<section >
	  <section data-background="https://upload.wikimedia.org/wikipedia/commons/3/3f/Craps_game_at_military_camp_in_1918.jpg">
	    <h2 class="backbox">A craps example</h2>
	    <p class="backbox fragment">
	      In craps, players roll two dice and place bets on the
	      outcome of the sum, which we'll call $U$.
	    </p>
	    <p class="backbox fragment">
	      We will later generalise to hyper-craps, where $N$ dice
	      are rolled simultaneously, but for now we start with
	      traditional craps where $N=2$.</p>	      
	  </section>
	  <section data-background="https://upload.wikimedia.org/wikipedia/commons/3/3f/Craps_game_at_military_camp_in_1918.jpg">
	    <p class="backbox">
	      The numbers on each individual dice at any point in the game/<b>simulation</b>
	      are called the <b>microstate</b> variables and describe everything about the <b>state</b> of
	      the game.</p>
	    <p class="backbox fragment" data-fragment-index="1" style="padding-top:0.5em">
	      <b>State</b> variables are observables which must change IFF<sup>1</sup> the system
	      changes.
	    </p>
	    <p class="backbox footnote fragment"  data-fragment-index="1">
	      1: IFF = IF and only if.</p>
	    <p class="backbox fragment">	      
	      The <b>microstate</b> of <em>molecular</em> systems are the atomic
	      coordinates and velocities.
	    </p>
	  </section>
	  <section data-background="https://upload.wikimedia.org/wikipedia/commons/3/3f/Craps_game_at_military_camp_in_1918.jpg">
	    <p class="backbox">
	      Players/<b>observers</b> of craps do not really care or measure the
	      <b>microstate</b>. The only <b>state</b> that is observed in craps is
	      the sum of the dice, $U$. The microstate could change, but if $U$
	      stays the same then as far as the players are concerned its the same
	      roll.</p>
	    <p class="backbox fragment">
	      The sum of the dice, $U$, is a <b>macrostate</b> variable
	      (along with $N$). A <b>macrostate</b> can consist of an <b>ensemble</b> of
	      many microstates. </p>
	    <p class="backbox fragment">
	      For example, the <b>state</b> of a sum of 7 on the two dice is made of the six <b>microstates</b>, $\left\{\left[1,\,6\right],\,\left[2,\,5\right],\,\left[3,\,4\right],\ldots\right\}$ .
	    </p>
	    <p class="backbox fragment">
	      This is like a molecular system, where an observer sees a particular <b>state</b> (i.e. pressure,
	      temperature, and mass) will have many possible <b>microstates</b> (molecular
	      configurations), and they don't particularly care about
	      the microstate.</p>
	    <p class="backbox fragment">
	      Note: When we say <b>state</b> we typically mean the <b>macrostate</b>
	      as its the one we can see experimentally/IRL, thus it's
	    the one we interact with and actually care about.</p>
	  </section>
	  <section data-background="https://upload.wikimedia.org/wikipedia/commons/3/32/Craps.jpg">
	    <p class="backbox">
	      <loaded>Fundamental postulate: Each <b>microstate</b> is equally probable.</loaded>
	    </p>	    
	    <p class="backbox fragment">
	      This is intuitive for perfect dice, each possible roll
	      is equally probable.  Not intuitive at all for molecular
	      systems; however, it works, and that is all we have time
	      for.</p>
	    <p class="backbox fragment">
	      Even though the <b>microstates</b> are equally probable, the <b>macrostates</b> have
	      different probabilities due to the <em>combinations</em> of <b>microstates</b> that make up their <b>ensemble</b>.
	    </p>	      
	    <p class="backbox fragment">
	      For two dice, rolling a $U=7$ <br/> 
	      $\left\{\left[1,\,6\right],\,\left[2,\,5\right],\,\left[3,\,4\right],\,\left[4,\,3\right],\,\left[5,\,2\right],\,\left[6,\,1\right]\right\}$.
	      <br/>
	      is six times more likely than a snake eyes ($U=2$)
	      <br/>
	      $\left\{\left[1,\,1\right]\right\}$
	    </p>
	    <p class="backbox fragment">
	      What happens as we add more dice? Molecular systems have
	      very large numbers of microstate variables
	      $\left(\mathcal{O}\left(10^{26}\right)\right)$, so we
	      need an intuition of what happens when we have so many
	      dice....</p>
	  </section>
	  <section style="height:100%" data-background="img/dice_heap.jpg" id="dicecount1slide">
	    <h2 class="backbox">Example: Hyper-craps</h2>
	    <form class="backbox">
	      <div class="row">
		<div class="col">
		  <div class="form-group">
		    <label id="dicecount1label" for="dicecount1">$N$: 1</label>
		    <input type="range" class="form-control-range" id="dicecount1" min="1" max="28" value="1" step="1">
		  </div>
		</div>
		<div class="col">
		  <input class="form-check-input" type="checkbox" value="" id="dicenormalisation1">
		  <label class="form-check-label" for="dicenormalisation1">
		    Per-dice normalisation
		  </label>
		</div>
	      </div>
	      <div class="row">
		<div class="col">
		  <input class="form-check-input" type="radio" name="dicey" id="dicey1" value="1" checked><br/>
		  <label class="form-check-label" for="dicey1" id="dicey1label" style="font-size:75%">$\Omega\left(N,\,U\right)$</label>
		</div>
		<div class="col">
		  <input class="form-check-input" type="radio" name="dicey" id="dicey2" value="2"><br/>
		  <label class="form-check-label" for="dicey2" id="dicey2label" style="font-size:75%">$P(U)=\Omega\left(N,\,U\right) / \sum_U \Omega\left(N,\,U\right)$</label>
		</div>
		<div class="col">
		  <input class="form-check-input" type="radio" name="dicey" id="dicey3" value="3"><br/>
		  <label class="form-check-label" for="dicey3" id="dicey3label" style="font-size:75%">$P(U)/\left({\rm max}_U\,P(U)\right)$</label>
		</div>
	      </div>
	    </form>
	    <div id="dicePlot" class="plot"/>
	    <div class="attribution backbox">
	      Background: <i><a href="/wiki/User:XRay" title="User:XRay">Dietmar Rabich</a>&nbsp;/ <a href="/wiki/Main_Page" title="Main Page">Wikimedia Commons</a>&nbsp;/ <span class="plainlinks noprint"><a class="external text" href="https://commons.wikimedia.org/wiki/File:W%C3%BCrfel,_gemischt_--_2021_--_5649.jpg">“Würfel, gemischt -- 2021 -- 5649”</a></span>&nbsp;/ <span class="plainlinks noprint"><a rel="nofollow" class="external text" href="https://creativecommons.org/licenses/by-sa/4.0/">CC&nbsp;BY-SA&nbsp;4.0</a></span></i>
	    </div>
	    <script>

	     function binomial(n, k) {
	       if ((typeof n !== 'number') || (typeof k !== 'number')) 
		 return false; 
	       var coeff = 1;
	       for (var x = n-k+1; x <= n; x++) coeff *= x;
	       for (x = 1; x <= k; x++) coeff /= x;
	       return coeff;
	     }

	     //https://mathworld.wolfram.com/Dice.html
	     function most_probable_rolls(dice_rolled, dice_sides) {
	       var n = dice_rolled;
	       var s = dice_sides;
	       if (dice_rolled == 1)
		 //A complex way of generating [1,2,3,4,..., dice_sides]
		 return [...Array(dice_sides).keys()].map((e) => {return e+1;});

	       var val = Math.floor(0.5 * n * (s + 1));
	       if (dice_rolled % 2)
		 return [val, val+1];
	       else
		 return [val];
	     }
	     
	     function dice_prob(dice_points, dice_rolled, dice_sides) {
	       var p = dice_points;
	       var n = dice_rolled;
	       var s = dice_sides;
	       
	       var N = Math.floor((p - n) / s);
	       var sum = 0;
	       for (var k=0; k <= N; ++k)
		 sum += ((-1)**k) * binomial(n, k) * binomial(p - s*k - 1, n-1);
	       return (dice_sides**(-dice_rolled)) * sum;
	     }

	     function dice_dist_plot(n, s) {
	       var x = [];
	       var y = [];
	       for (var k = n; k <= n * s; ++k) {
		 x.push(k);
		 y.push(dice_prob(k, n, s));
	       }
	       return {x, y};
	     }
	     
	     var max_dice = parseInt($('#dicecount1').max);
	     var dice_sides = 6;
	     
	     function refresh_dice_plot(dice_rolled) {
	       var mpr = most_probable_rolls(dice_rolled, dice_sides);
	       
	       var normalise_x = 1;
	       if ($('#dicenormalisation1')[0].checked)
		 normalise_x = dice_rolled;
	       var ystate = parseInt($('input[name=dicey]:checked').val());
	       
	       var data1 = dice_dist_plot(dice_rolled, dice_sides);
	       data1.x = data1.x.map((e) => { return e / normalise_x; });

	       var data2 = {
		 x: mpr.map((e) => { return e / normalise_x; } ),
		 y: mpr.map((element) => { return dice_prob(element, dice_rolled, dice_sides); }),
	       };

	       if (ystate == 1) {
		 var factor = data1.y[0];
		 data1.y = data1.y.map((e) => { return e / factor; });
		 data2.y = data2.y.map((e) => { return e / factor; });
	       } else if (ystate == 2) {
		 
	       } else if (ystate == 3) {
		 data1.y = data1.y.map((e) => { return e / data2.y[0]; });
		 data2.y = data2.y.map((e) => { return 1.0; });		 
	       }

	       var xlabel = 'U';
	       
	       if ($('#dicenormalisation1')[0].checked) 
		 xlabel = 'u';
	       
	       var traces = [
		 mergeDeep(
		   {
		     name:'Value',
		     type:'bar',
		   },
		   data1
		 ),
		 {
		   name:'Peak',
		   mode: 'markers',
		   type: 'scatter',
		   ...data2,
		 },
	       ];

	       var layout = {
		 showlegend:false,
		 xaxis: {
		   title: '$'+xlabel+'$',
		   autorange: true,
		 },
		 yaxis: {
		   title: {
		     text: '$P\\left(' + xlabel + '\\right)$',
		   },		   
		   range:[0, 1.0 / dice_sides + 0.1],
		 }
	       };
	       
	       
	       if (ystate == 1) {
		 layout.yaxis.autorange = true;
		 layout.yaxis.title = '$\\Omega\\left(N,\\,'+ xlabel +'\\right)$';
	       } else if (ystate == 2) {
		 layout.yaxis.autorange = false;
		 layout.yaxis.range = [0, 1.0 / dice_sides + 0.1];
		 layout.yaxis.title = '$P\\left(' + xlabel + '\\right)$';
	       } else if (ystate == 3) {
		 layout.yaxis.autorange = false;
		 layout.yaxis.range = [0, 1.0];
		 layout.yaxis.title = '$P\\left(' + xlabel + '\\right) / \\left(\\max_{'+xlabel+'} P\\left(' + xlabel + '\\right)\\right)$';
	       }

	       return {
		 data: traces,
		 layout: mergeDeep(default_plot_Layout, layout),
		 config: default_plot_Config,
	       };
	     }
	     

	     function diceupdate1() {
	       var target = $('#dicecount1')[0];
	       $('#dicecount1label').text("N: "+target.value);
	       Plotly.react('dicePlot', refresh_dice_plot(target.valueAsNumber, dice_sides));
	     }

	     $('#dicecount1').on('input', (event) => { diceupdate1() });
	     $('#dicenormalisation1').on('input', (event) => { diceupdate1() });
	     $('input[name=dicey]').on('input', (event) => { diceupdate1() });


	     Reveal.on( 'slidechanged', event => {
	       if (event.currentSlide.id == 'dicecount1slide') {
		 Plotly.plot('dicePlot', refresh_dice_plot(1, 6));
		 Reveal.layout();		 
	       } else {
		 Plotly.purge('dicePlot');
	       }
	     });	       
	     
	    </script>
	  </section>
	  <section style="height:100%; width:100%;">
	    <div style="height:90%;width:100%; position:relative;">
	      <iframe src="http://www.youtube.com/embed/03tx4v0i7MA" frameborder="0" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
	    </div>
	  </section>
	  <section class="backbox">
	    <h3 class="backbox">
	      Key points from hyper-craps
	    </h3>
	    <ul >
	      <li class="fragment">
		Combinations increase incredibly quickly, even for
	      small systems (20 dice, 189T combinations to roll 70).</li>
	      <li class="fragment">
		The distribution sharpens as $N\to\infty$. Thus
		&ldquo;large&rdquo; systems will have a relatively
		small collection of extremely-high probability states,
		which we will call the <b>equilibrium state</b>.
	      </li>
	      <li class="fragment">
		This equilibrium state <loaded>might be &ldquo;unique&rdquo;</loaded>
		and appear at a maximum (more on uniqueness later).
	      </li>
	      <li class="fragment">
		Everything is possible, but much of it is highly
		highly highly highly improbable compared to the
		equilibrium state.</li>
	      <li class="fragment">
		Normalisation is a convenience to:
		<ul>
		  <li>Make properties <b>intensive</b> (i.e. $U$ to $u$).</li>
		  <li>Move from combinations to probability, which is more useful for calculations.</li></ul>
	      </li>
	      <li class="fragment">
		The number of microstates, $\Omega(N,\,U)$, for a
		particular dice roll/macrostate, $U$, is an
		unnormalised probability.</li>
	    </ul>
	  </section>
	</section>
	<section>
	  <section data-background="img/Boltzmann_grave.jpg">
	    <h3 class="backbox">Thermodynamics of craps</h3>
	    <p class="backbox">
	      Lets say we start with a roll of hyper-snake-eyes (all
	      ones). Its <b>density-of-states</b>/combinations is the lowest possible value in this system of $\Omega(N,\, U=N)=1$.
	      What happens if we start to reroll dice randomly one at
	      a time?</p>
	    <p class="backbox fragment">
	      The system moves towards states with higher $\Omega$,
	      simply because they are more probable. This movement is
	      gradual as we're rerolling small portions of dice, thus
	      introducing quasi-dynamics.</p>
	    <p class="backbox fragment">
	      Generally ${\rm d}\Omega \ge 0$..... just like entropy!
	    </p>
	    <div class="attribution backbox">
	      Background <a href="https://en.wikipedia.org/wiki/Ludwig_Boltzmann#/media/File:Zentralfriedhof_Vienna_-_Boltzmann.JPG">Daderot at English Wikipedia</a> - Own work CC BY-SA 3.0
	    </div>
	  </section>
	  <section style="height:100%" data-background="img/Boltzmann_grave.jpg">
	    <h3 class="backbox">Thermodynamics of craps</h3>
	    <p class="backbox">
	      But entropy is additive, i.e. $2\times$ the system size
	      gives $2\times$ the entropy. Can combinations match this property?</p>
	    <p class="backbox fragment">
	      \begin{multline}
	      \Omega\left(N=N_1+N_2,\,U=U_1+U_2\right) \\
	      = \Omega\left(N_1,\,U_1\right)\times\Omega\left(N_2,\,U_2\right)
	      \end{multline}
	    </p>
	    <p class="backbox fragment">
	      \begin{multline}
	      \ln\Omega\left(N=N_1+N_2,\,U=U_1+U_2\right) \\
	      = \ln\Omega\left(N_1,\,U_1\right) + \ln\Omega\left(N_2,\,U_2\right)
	      \end{multline}
	    </p>
	    <p class="backbox fragment">
	      $S = \ln \Omega$ has all the properties of entropy! We
	      have discovered it inside our game of craps, it is not
	      too much of a leap to believe it is more fundamental and
	      in molecular systems too.</p>
	    <div class="attribution backbox">
	      Background <a href="https://en.wikipedia.org/wiki/Ludwig_Boltzmann#/media/File:Zentralfriedhof_Vienna_-_Boltzmann.JPG">Daderot at English Wikipedia</a> - Own work CC BY-SA 3.0
	    </div>
	  </section>
	  <section style="height:100%" data-background="img/Boltzmann_grave.jpg">
	    <h3 class="backbox">Thermodynamics of everything</h3>
	    <p class="backbox">
	      Due to a historical accident, the actual relationship to
	      entropy is multiplied by the Boltzmann constant, $k_B$.
	      <img src="img/Boltzmann_equation.jfif"><br/>
	      where $W=\Omega$, and $k.=k_B$.
	    </p>
	    <p class="backbox fragment">
	      We now intuitively understand entropy as some measure of
	      probability, thus it increases as things move
	      towards the most probable state... but only probably (!).</p>
	    <p class="backbox fragment">
	      This has fascinating implications, i.e. see<a href="https://www.bbc.co.uk/sounds/play/m000lvbx">
	      The Infinite Monkey Cage: Does Time Exist?"
	      </a> for one aspect.
	    </p>
	    <div class="attribution backbox">
	      Background <a href="https://en.wikipedia.org/wiki/Ludwig_Boltzmann#/media/File:Zentralfriedhof_Vienna_-_Boltzmann.JPG">Daderot at English Wikipedia</a> - Own work CC BY-SA 3.0
	    </div>
	  </section>
	  <section data-background-image="img/mindblown.gif">
	    <p class="backbox">
	      The second law of thermodynamics, put simply, is that
	      the most likely things will eventually happen and stay
	      there.
	    </p>
	    <p class="backbox fragment">
	      Richard Feynman says in his Lectures on Physics “…
	      entropy is just the logarithm of the number of ways of
	      internally arranging a system while <loaded>have it look the same
	      from the outside</loaded>”.</p>
	    <p class="backbox fragment"> 
	      Entropy is a measure of how much we don't know, i.e. it
	      measures our lack of control-over/measurement-of the
	      microstate.</p>
	    <p class="backbox fragment">
	      Entropy is NOT disorder/chaos, its just that there's so many
	      ways to make a mess... 
	    </p>
	    <p class="backbox fragment"> 
	      Sometimes order is more likely, i.e., look at any
	      crystal transition.
	    </p>
	  </section>
	  <section style="height:100%">
	    <h3 class="backbox">Microcanonical ensemble</h3>
	    <p class="backbox">
	      We have derived the key relationships of the <b>micro-canonical ensemble</b>,
	      where $N, U$ are held constant (typically $N,\,V,\,E$ in molecular systems):
	      <span style="font-size:65%">
	      \begin{align*}
	      P(N,\,U, \ldots) &= \frac{\Omega\left(N, U, \ldots\right)}{\sum_U \Omega\left(N,\,U, \ldots\right)}
	      &
	      S(N,\,U, \ldots) &= k_B \ln \Omega\left(N, U, \ldots\right)
	      \end{align*}
	      </span>
	    </p>
	    <p class="backbox fragment"> 
	      Other thermodynamic properties can be generated from
	      here, just like in normal thermodynamics
	      
	      <span style="font-size:65%">		
	      \begin{align*}
	      \left\langle U\right\rangle &= \sum_U U\,P(N,\,U, \ldots) &
	      \mu &= - T\frac{\partial S(N,\,U, \ldots)}{\partial N}
	      \end{align*}	      
	      </span>
	    </p>
	    <p class="backbox fragment">
	      We don't have pressure or any other work variable in this system, we'd
	      need to have some "volume" and "pressure" that somehow contributes to
	      the dice roll for that.
	    </p>
	    <p class="backbox fragment">
	      However, we do still have temperature, although it requires us to add
	      the first law.
	    </p>
	  </section>
	  <section>
	    <h3>Canonical ensemble</h3>
	    <p>
	      Lets hold the dice sum, $U$, fixed in ALL our dice
	      rolls. If you haven't guessed yet from the variable
	      name, we're adding conservation of energy. </p>
	    <p class="fragment">
	      Divide the $N$ dice into two groups, each with a
	      separate $U_1$ and $U_2 = U - U_1$ which adds to the
	      (fixed) total $U=U_1+U_2$.
	      <img src="img/NEDiceEnsemble.svg" style="width:75%" />
	    </p>
	  </section>
	    <section>
	      <h3>Canonical ensemble</h3>
	      <p >
	      The equilibrium value of $U_1$ occurs at the maxiumum
	      entropy, where the total entropy is:
	      
	      \begin{align*} \ln \Omega_{1+2}(U_1+U_2) &= \ln
	      \Omega_1(U_1) + \ln \Omega_2(U_2) \\ \ln
	      \Omega_{1+2}(U,\,U_1) &= \ln \Omega_1(U_1) + \ln
	      \Omega_2(U-U_1) \\ \end{align*}

	      The maximum entropy when varying $U_1$ (but holding $U$ fixed) occurs
	      at a stationary point, i.e.  
		
		\begin{align*}
		\left(\frac{{\rm d} \ln
		\Omega_{1+2}(U,\,U_1)}{{\rm d} U_1}\right)_{N} = 0
		\end{align*}
	      </p>
	  </section>
	  <section>
	    <h3>The rest of thermodynamics</h3>
	    <p>
	      \begin{align*}
	      \frac{{\rm d}}{{\rm d} U_1}\ln \Omega_{1+2}(U,\,U_1) &= 0\\
	      \frac{{\rm d}}{{\rm d} U_1}\ln \Omega_1(U_1) + \frac{{\rm d}}{{\rm d} U_1}\ln \Omega_2(U-U_1)&=0 \\
	      \frac{{\rm d}}{{\rm d} U_1}\ln \Omega_1(U_1) &= \frac{{\rm d}}{{\rm d} U_2}\ln \Omega_2(U_2)
	      \end{align*}
	    </p>
	    <p class="fragment">
	      We now define some shorthand: \begin{align*} \beta = \frac{{\partial}
	      S\left(N,\,U,\,\ldots\right)}{{\partial} U} = \frac{{\partial} \ln
	      \Omega\left(N,\,U,\,\ldots\right)}{{\partial} U} \end{align*} and note
	      the result, $\beta_1 = \beta_2$ when two systems have fixed energy and
	      are at maximum entropy/equilibrium.
	    </p>
	  </section>
	  <section>
	    <h3>Back to reality</h3>
	    <p>
	      Thus adding conservation of $U$ between two systems that
	      can exchange ${\rm d}U$ gives us that $\beta$ must be
	      equal between the systems at equilibrium. We call this
	      &ldquo;thermal&rdquo; equilibrium if $U$ is energy, even though it
	      works for conserved dice sum too.</p>
	    <p class="fragment">
	      If we look up the derivative $\left({\rm d}S/{\rm
	      d}U\right)_{N,\ldots}$ in traditional thermodynamics, we
	      realise that $\beta=1/(k_B\,T)$, and thus have proven
	      that systems at thermal equilibrium must have the same
	      temperature.</p>
	    <p class="fragment">
	      This generalises to three systems, and thus is proof of the zeroth law.
	    </p>
	    <p class="fragment">
	      Lets now generate the probabilty function for when $U$ is conserved
	      (and everything else).
	    </p>
	  </section>
	  <section>
	    <h3>Canonical ensemble</h3>
	    <p class>
	      Again, consider system 1 in a <b>microstate</b>, $i$, with
	      energy $U_{1,i}$ in thermal equilibrium with another
	      system 2 at constant $U$. The combinations available are
	      set by system 2's <b>degeneracy</b>:
	      
	      <span style="font-size:65%">
		\begin{align*} 
		P\left(i\right) \propto \Omega_2(U_2=U-U_{1,i})
		\end{align*}
	      </span>
	    </p>
	    <p class="fragment">
	      Expanding system 2 entropy around $U$:
	      <span style="font-size:65%">
		\begin{align*}
		\ln \Omega_2(U_2=U-U_{1,i}) &= \ln\Omega_2(U_2=U) 
		- U_{1,i} \frac{{\rm d} \ln\Omega_2(U)}{{\rm d}U} 
		+ \mathcal{O}(1/U)
		\\
		&\approx \ln\Omega_2(U_2=U) - \frac{U_{1,i}}{k_B\,T}
		\end{align*}
	      </span>
	    </p>
	    <p class="fragment">
	      Assuming system 2 is very large so that changes in $U_1$ become negligble to it, giving $U_2\to U$, substituting, and
	      realising that $\ln\Omega_2(U_2=U)$ is a constant,
	      
	      <span style="font-size:65%">		
		\begin{align*} 
		P\left(i\right) \propto e^{-U_{1,i}/(k_B\,T)}
		\end{align*}
	      </span>
	    </p>
	  </section>
	  <section>
	    <h3>Canonical ensemble</h3>
	    <p>
	      Each <b>microstate</b> of a system at thermal
	      equilibrium (i.e. fixed $N,\,T,\ldots$ otherwise known
	      as the canonical ensemble) has a probability
	      proportional to the Boltzmann factor:
	      
	      \begin{align*} P\left(i\right) \propto e^{-U_{1,i} /
	      (k_B\,T)} \end{align*}</p>
	    <p class="fragment">
	      We can normalise this by creating the so-called <b>canonical partition</b> function,
	      \begin{align*} Z(N,\,T) &= \sum_{i} e^{-U_{1,i} / (k_B\,T)} \\
	      P\left(i\right) &= Z^{-1} e^{-U_{i} / (k_B\,T)}
	      \end{align*}

	      We've dropped the system 1 and 2 notations, as system 2
	      (called the &ldquo;bath&rdquo;) is irrelevant (despite its enormous size), aside
	      from its temperature which is also the temperature of
	      system 1 (at equilibrium).</p>
	  </section>
	  <section>
	    <h3>Canonical partition function</h3>
	    <p>
	      <span style="font-size:75%">
		\begin{align*} Z(N,\,T) &= \sum_{i} e^{-U_{1,i} /
		(k_B\,T)} & P\left(i\right) &= Z^{-1} e^{-U_{i} /
		(k_B\,T)} \end{align*}
	      </span>
	      Like the relationship for entropy,
	      $S=k_B\ln\Omega(N,\,U,\ldots)$, where its the log of the
	      probability normalisation for $\left(N,\,U\right)$
	      ensembles, the partition function in $\left(N,\,T\right)$ ensembles
	      is related to the <b>Helmholtz free energy</b>, $F$:
	      
	      <span style="font-size:75%">
		\begin{align*}
		F(N,\,T,\ldots) &= k_B\,T\ln Z(N,\,T,\ldots)
		\end{align*}
	      </span>
	    </p>
	    <p class="fragment">
	      And this is the generating function for all thermodynamic properties of this ensemble:
	      <span style="font-size:75%">
		\begin{align*}
		S &= -\frac{\partial F(N,\,T,\ldots)}{\partial T} & \mu &= \frac{\partial F(N,\,T,\ldots)}{\partial N}\\
		F &= U-T\,S & \ldots 
		\end{align*}
	      </span>
	    </p>
		<p>
			This applies to dice, but also molecules. But now we must leave dice behind and add additional work terms.
		</p>
	  </section>
	  <section>
	    <h3>Other ensembles</h3>
	    <p>
	      By using similar arguments of dividing an ensemble of
	      systems, allowing them to exchange something, performing a
	      Taylor series, and comparing against macroscopic
	      thermodynamics we can generate other ensembles.
	    </p>
	    <p class="fragment">
	      <b>Isothermal-Isobaric ensemble $(N,\,T,\,p)$</b>
	      
	      <span style="font-size:75%">
		\begin{align*}
		P\left(i,\,N,\,p,\,T,\ldots\right) &= \mathcal{Z}^{-1} e^{-U_{i}/\left(k_B\,T\right)-p\,V_i/\left(k_B\,T\right)}
		\\
		G\left(N,\,p,\,T,\ldots\right) &= k_B\,T\ln \mathcal{Z}\left(N,\,p,\,T,\ldots\right)
		\end{align*}
	      </span>
	    </p>
	    <p class="fragment">
	      <b>Grand-canonical ensemble $(\mu,\,T,\,p)$</b>
	      
	      <span style="font-size:75%">
		\begin{align*}
		P\left(i,\,\mu,\,p,\,T,\ldots\right) &= \mathcal{Z}^{-1} e^{-U_{i}/\left(k_B\,T\right)-p\,V_i/\left(k_B\,T\right)+\mu\,N_i/\left(k_B\,T\right)}
		\\
		\boldsymbol{\Omega}\left(\mu,\,p,\,T,\ldots\right) &= k_B\,T\ln \mathcal{Z}\left(N,\,p,\,T,\ldots\right)
		\end{align*}
	      </span>
	    </p>
	    <p class="fragment">
	      Calculating move probabilities is fundamental to Monte
	      Carlo, thus these probabilities (and how to compute the
	      partition functions) is core to that approach.
	    </p>
	  </section>
	  <section>
	    <h3>Outlook</h3>
	    <p class="backbox">
	      There are too many important derivations to cover in
	      stat mech, and what you will need highly depends on your
	      interests/research. We cannot do more here, but please ask if
	      you have something in particular to understand in the
	      problem sessions.
	    </p>
	    <p class="fragment backbox">
	      I will now try to give you some intuition on the other
	      foundational assumption of stat mech (which I think is
	      much harder to find).</p>
	  </section>
	</section>
	<section>
	  <section>
	    <h3>Averaging</h3>
	    <p>
	      <em>God does not play dice. <span style="float:right;">A. Einstein</span></em>
	    </p>
	    <p style="clear:right;" class="fragment">
	      <em>God does play dice with the universe. All the evidence points to him being an inveterate gambler, who throws the dice on every possible occasion. <span style="float:right;">S. Hawking</span></em>
	    </p>
	    <p class="fragment">
	      You are the omnipotent being of your simulation, so you get to choose:
	      <span style="font-size:75%">
		\begin{align*}
		\langle{\mathcal A}\rangle &= \sum_{\boldsymbol\Gamma} d{\boldsymbol\Gamma}\,P({\boldsymbol\Gamma})\,
		{\mathcal A}({\boldsymbol\Gamma}) & \text{(Monte Carlo)}\\
		&\text{or}\\
		\langle{\mathcal A}\rangle&=\lim_{t_{obs.}\to\infty} \frac{1}{t_{obs.}}\int_0^{t_{obs.}}\mathcal A(t)\,{\rm d}t & \text{(Molecular Dynamics)}
		\end{align*}
	      </span>
	    </p>	    
	    <p class="fragment">
	      <b>But are these equivalent? If so, the system is <u>Ergodic</u>.</b>
	    </p>
	  </section>
	  <section>
	    <h3>Dynamics for SHO</h3>
	    <p>
	      Consider the <b>S</b>imple <b>H</b>armonic <b>O</b>scillator:
	      <img src="img/SHO.gif"><br/>
	    </p>
	    <p class="fragment">
	      The <b>microstate</b> variables, in this case position,
	      $r$, and velocity, $v=\dot{r}$, can be collected together to
	      form a vector, $\boldsymbol \Gamma$, which exists in <b>phase space</b>.</p>
	    <div class="attribution backbox">
	      Image: 
	      <i><a href="https://en.wikipedia.org/wiki/Simple_harmonic_motion#/media/File:Simple_Harmonic_Motion_Orbit.gif" title="User:XRay">Mazemaster - public domain</a>
	      </i>
	    </div>
	  </section>
	  <section id="shocount1slide">
	    <p>
	      The <b>microcanonical</b> SHO system visits all regions of phase space pretty quickly, it is therefore <b>Ergodic</b>. Stat-Mech
	      and (molecular) dynamics averages are equivalent.</p>
	    <div class="backbox">
	      <div class="form-group">
		<label for="shocount1">Ensemble members:</label><output id="shocount1output" for="shocount1">1</output>
		<input type="range" class="form-control-range" id="shocount1" min="1" max="500" value="1" step="1">
	      </div>
	    </div>
	    <div id="shocount1div" style="margin:auto 0; display:inline-block;" ></div>
	    <script>
	     (function() {
	       var state = {
	       };

	       function init(n) {
		 state.n = n;
		 state.dt = 0.015;
		 state.x = [];
		 state.y = [];
		 state.tinit = [];
		 state.A = [];
		 state.t = 0;
		 
		 for (i = 0; i < state.n; i++) {
		   state.tinit[i] = Math.random();
		   state.A[i] = 1.0;
		 };
	       };

	       function step() {
		 state.t += state.dt;
		 for (i = 0; i < state.n; i++) {
		   var phase = 2 * 3.14159265 * (state.tinit[i] + state.t);
		   state.x[i] = state.A[i] * Math.sin(phase);
		   state.y[i] = state.A[i] * Math.cos(phase);
		 };
	       };

	       init(1);
	       step();

	       $('#shocount1').on('input change', 
				  (e) => {
				    $('#shocount1output').text(e.target.value);
				    init(e.target.valueAsNumber); 
	       });
	       
	       var animating = false;

	       function update () {
		 step();

		 Plotly.animate('shocount1div', {
		   data: [{
		     x: state.x, 
		     y: state.y, 					
		     marker: {
		       color: [...Array(state.x.length).keys()],
		       cmin: 0,
		       cmax:state.x.length,
		     },
		   }],
		 }, {
		   transition: {
		     duration: 0
		   },
		   frame: {
		     duration: 0,
		     redraw: false
		   }
		 });
		 
		 if (animating)
		   requestAnimationFrame(update);
		 else {
		   Plotly.purge('shocount1div');
		 }
	       }

	       Reveal.on( 'slidechanged', event => {
		 if (event.currentSlide.id == 'shocount1slide') {
		   Plotly.newPlot('shocount1div', 
				    {
				      data:	[{
					x: state.x,
					y: state.y,
					marker: {
					  color: [...Array(state.x.length).keys()],
					  colorscale: 'Rainbow',
					},
				      mode: 'markers',
				    }],
				    layout:  { 
				      xaxis: { title: 'r', range: [-1.5, 1.5]},
				      yaxis: { title: 'v', range: [-1.5, 1.5]},
				      plot_bgcolor: '#000',
				      paper_bgcolor: '#000E',
				      font: {
					color: 'white',
					size: 15,
				      },
				    },
				    config: default_plot_Config,
		   });
		   Reveal.layout();
		   animating = true;
		   requestAnimationFrame(update);
		 } else {
		   animating = false;
		 }
	       });
	     })();
	    </script>
	  </section>
	  <section>
	    <h3>SHO Hamiltonian</h3>
	    <p>
	      Its not surprising that it forms a circle in phase space,
	      as the total energy (called the Hamiltonian), is the
	      equation of a circle in $r$ and $v$.
	      
	      \begin{align*}
	      U = \mathcal{H}(\boldsymbol{\Gamma}) = \frac{1}{2}k\,r^2 + \frac{1}{2}m\,v^2
	      \end{align*}
	      where $k$ is the spring constant and $m$ is the mass. 
	    </p>
	    <p class="fragment">
	      What is all of phase space like? We can sample all energies, $U$, and plot the "flow" of trajectories:
	    </p>
	  </section>
	  <section>
	    <h3>SHO phase space &ldquo;flow&rdquo;</h3>
	    <div style="display:flex">
	      <img src="img/SHOflow1.svg" style="flex:1" />
	      <div style="flex:1" >
		<p style="padding-left:1em;">
		  The systems flow around phase space, the trajectory
		  lines do not cross, thus this behaves exactly as a
		  incompressible fluid.
		</p>
		<p class="fragment" style="padding-left:1em;">
		  Whatever <b>ensemble</b> of starting states we begin
		  with, there's a symmetry to its motion through phase
		  space. If evenly distributed, it will remain evenly
		  distributed.
		</p>
	      </div>
	    </div>
	  </section>
	  <section>
	    <h3>Liouville's Theorem</h3>
	    <p>
	      We can treat an ensemble of systems flowing around phase
	      space as an incompressible fluid with density
	      $\rho(\boldsymbol{\Gamma})$, thus it behaves as follows.
	      
	      <span style="font-size:75%">
		\begin{align*} \nabla_{\boldsymbol{\Gamma}} \cdot
		\dot{\boldsymbol\Gamma}\,\rho({\boldsymbol\Gamma}) = \sum_i\left(\dot{\boldsymbol{r}}_i\cdot \frac{\partial}{\partial \boldsymbol{r}_i} + \dot{\boldsymbol{v}}_i \cdot \frac{\partial}{\partial \boldsymbol{v}_i}\right)\rho\left(\left\{\boldsymbol{r}_i,\,\boldsymbol{v}_i\right\}^N_i\right) = 0
		\end{align*}
	      </span>
	      which should be familiar to you as a simplification of
	      the continuity equation from any fluid flow class you
	      might have had:
	      
	      <span style="font-size:75%">
		\begin{align*} \frac{\partial \rho}{\partial t} +\nabla
	      \cdot \rho\,\boldsymbol{v} =0 \end{align*}
	      </span>
	    </p>
	    <p class="fragment">
	      This is the foundation of kinetic theory, as from here
	      comes the BBGKY heirarchy and ultimately
	      Boltzmann's/Enskog's equation.
	    </p>
	    <p class="fragment">
	      Back to ergodicity.
	    </p>
	  </section>
	  <section>
	    <h3>Double well potential</h3>
	    <p>
	      The SHO system has a parabolic potential energy:

	      \begin{align*}
	      U(\boldsymbol{\Gamma}) = \mathcal{H}(r,\,v) = \frac{1}{2}k\,r^2 + \frac{1}{2}m\,v^2
	      \end{align*}
	    </p>
	    <p class="fragment">
	      We can make a double-well potential simply by shifting the spring rest position away from the origin:
	      
	      \begin{align*}
	      U(\boldsymbol{\Gamma}) = \mathcal{H}(r,\,v) = \frac{1}{2}k\,\left(\left|r\right|-r_0\right)^2 + \frac{1}{2}m\,v^2
	      \end{align*}
	      
	      <span style="font-size:50%">
		This potential is not a particularly nice one to
		integrate as it is discontinuous at $r=0$. Even though
		I use velocity verlet (which is symplectic) as the
		integrator in the next example, you can still see the
		system drifting in energy due to where/when the time
		step crosses the $r=0$ line. But the demonstration is
		quite beautiful so I claim artistic licence, whatever
		that means.</span></p>
	  </section>
	  <section id="shocount2slide">
	    <div class="backbox">	
	      <div class="row">
		<div class="col">
		  <div class="form-group">
		    <label for="shocount2">Ensemble members: </label><output id="shocount2output" for="shocount2">1</output><br/>
		    <input type="range" class="form-control-range" id="shocount2" min="1" max="500" value="200" step="1">
		  </div>
		</div>
		<div class="col">
		  <div class="form-group">
		    <label for="shocount2dt">Time step: </label><output id="shocount2dtout" for="shocount2dt">1</output><br/>
		    <input type="range" class="form-control-range" id="shocount2dt" min="0.005" max="0.4" value="0.2" step="0.005">
		  </div>
		</div>
	      </div>
	    </div>
	    <div id="shocount2div" style="margin:auto 0; display:inline-block;" ></div>
	    <p>
	      Are the systems in the lower energies (closer to the centre of the
	      orbits) ergodic? No, they cannot visit each other even though they're
	      at the same energy level (thus at the same state)!
	    </p>
	    <script>
	     (function() {
	       var state = {
		 x: [],
		 y: []
	       };
	       var beta = 0; 
	       var omega = 1;
	       var gamma = 2 * beta;
	       var k = omega * omega + gamma * gamma / 4;
	       var m = 1
	       var r0 = 1;


	       function init() {
		 state.n = $('#shocount2')[0].valueAsNumber;
		 $('#shocount2output').text(state.n.toString());
		 state.dt = $('#shocount2dt')[0].valueAsNumber;
		 $('#shocount2dtout').text(state.dt.toString());
		 
		 if (state.n != state.x.length) {
		   state.x = [];
		   state.y = [];
		   for (i = 0; i < state.n; i++) {
		     state.x[i] = 3.0 * 2 * (Math.random() - 0.5);
		     state.y[i] = 3.0 * (Math.random() - 0.5);
		   };
		 }
	       };

	       function accel(i) {
		 var ri = state.x[i];
 		 var vi = state.y[i];
		 return - k * (Math.abs(ri) / ri) * (Math.abs(ri) - r0);
	       }

	       function step() {
		 for (i = 0; i < state.n; i++) {
		   var ai  = accel(i);
		   state.x[i] += state.dt * state.y[i] + 0.5 * ai * state.dt * state.dt;
		   state.y[i] += 0.5 * state.dt * ai;
		   var ai  = accel(i);
		   state.y[i] += 0.5 * state.dt * ai;
		 };
	       };

	       init();
	       step();

	       $('#shocount2').on('input change', 
				  (e) => {
				    init(); 
	       });

	       $('#shocount2dt').on('input change',
				  (e) => {
				    init(); 
	       });
	       
	       var animating = false;	       
	       function update () {
		 step();

		 Plotly.animate('shocount2div', {
		   data: [{
		     x: state.x, 
		     y: state.y,
		     marker: {
		       color: [...Array(state.x.length).keys()],
		       cmin: 0,
		       cmax:state.x.length,
		     },
		   }],
		 },{ 
		   transition: {
		     duration: 0
		   },
		   frame: {
		     duration: 0,
		     redraw: false
		   }
		 });

		 if (animating)
		   requestAnimationFrame(update);
		 else {
		   Plotly.purge('shocount2div')
		 }
	       }

	       Reveal.on( 'slidechanged', event => {
		 if (event.currentSlide.id == 'shocount2slide') {
		   Plotly.newPlot('shocount2div', 
				  {
				    data:	[{
				      x: state.x,
				      y: state.y,
				      mode: 'markers',
				      marker: {
					color: [...Array(state.x.length).keys()],
					colorscale: 'Jet',
					cmin: 0,
					cmax:state.x.length,
				      },
				    }],
				    layout:  { 
				      xaxis: { title: 'r', range: [-4, 4]},
				      yaxis: { title: 'v', range: [-4, 4]},
				      plot_bgcolor: '#000',
				      paper_bgcolor: '#000E',
				      font: {
					color: 'white',
					size: 15,
				      },
				    },
				    config: default_plot_Config,
		   });
		   Reveal.layout();
		   
		   animating = true;
		   requestAnimationFrame(update);
		 } else {
		   animating = false;
		 }
	       });	       
	     })();
	    </script>
	  </section>
	  <section>
	    <h3>$N,\,U$ double-well SHO dynamics</h3>
	    <img src="img/SHOsplit.svg" style="width:100%" />
	    <p>
	      <b>Microstates</b> in $U=0.25$ are energetically
	      isolated from each other, thus its not ergodic; however,
	      a lot of properties (i.e. average velocity) will still work out!</p>
	  </section>
	</section>
	<section>
	  <section>
	    <h3>The final word on ergodicity</h3>
	    <p>
	      Phase space is so unbelievably big even for small
	      systems it actually &ldquo;feels&rdquo; impossible for any
	      trajectory to meaningfully visit all microstates (or come
	      close enough to do so) in finite time.
	    </p>
	    <p class="fragment">
	      Practically, in simulation we typically just check
	      averages are converging and hope we're <loaded>effectively</loaded>
	      ergodic. Like a lot of things in simulation, you presume
	      it might be true but verify as best you can.</p>
	    <p class="fragment">
	      There are many techniques for accelerating dynamics past
	      energy barriers and &ldquo;encourage&rdquo; good sampling of phase space
	      which you will learn about in your MC lectures. These
	      are also applied more frequently to MD simulations too.</p>
	  </section>
	  <section>
	    <h3>The final word on statistical mechanics</h3>
	    <p class="backbox">
	      The ideas of statistical mechanics are simple, yet the
	      implications and relationships it provides are deep and
	      meaningful.
	    </p>
	    <p class="backbox fragment">
	      My dice example should make you believe that its
	      applications are wider than just molecular
	      systems. Isaac Asimov even explored the statistical
	      mechanics of galactic societies in his excellent (but a little
	      dated) <a href="https://en.wikipedia.org/wiki/Foundation_series">Foundation Series</a>. Who
	      knows where it will turn up next.
	    </p>
	    <p class="backbox fragment">
	      While much of the derivations covered here are
	      &ldquo;ancient&rdquo;, there's still intense research
	      into the field, and many more surprising results to
	      find.
	    </p>
	    <p class="backbox fragment">
	      I hope you're keen to find out more, and will look at
	      the statistical mechanics underlying all your upcoming
	      lectures as a mountain to climb up, rather than a chasm to
	      fall in.</p>
	  </section>
	  <section data-background-image="img/endframe.svg" data-background-size="contain">
	    <h3 style="background:none">Fin.</h3>
	  </section>
	</section>
	<section id="MC1">
	  <section>
	    <h2 class="backbox">Monte Carlo: Part 1</h2>
	    <p>
	      <em>God does not play dice. <span style="float:right;">A. Einstein</span></em><br/><br/>
	      <em style="clear:right;" class="fragment">God does play dice with the universe. All the evidence points to him being an inveterate gambler, who throws the dice on every possible occasion. <span style="float:right;">S. Hawking</span></em>
	  </section>
	  <section>
	    <p>
	      You are the omnipotent being of your simulation, so you get to choose:
	      \begin{align*}
	      \langle{\mathcal A}\rangle &= \int d{\boldsymbol\Gamma}\,{\mathcal P}({\boldsymbol\Gamma})\,
	      {\mathcal A}({\boldsymbol\Gamma}) & \text{(Monte Carlo)}\\
	      &\text{or}\\
	      \langle{\mathcal A}\rangle&=\lim_{t_{obs.}\to\infty} \frac{1}{t_{obs.}}\int_0^{t_{obs.}}\mathcal A(t)\,{\rm d}t & \text{(Molecular Dynamics)}
	      \end{align*}
	    </p>	    
	  </section>
	  <section>
	    <p>
	      Imagine simulating roulette to evaluate betting
	      strategies on your balance $\mathcal{A}$. You could evaluate
	      them over time $t$ by simulating the motion of the ball as
	      it visits the different numbers/states of the wheel,
	      
	      \begin{align*}
	      \langle{\mathcal A}\rangle&=\lim_{t_{obs.}\to\infty} \frac{1}{t_{obs.}}\int_0^{t_{obs.}}\mathcal A(t)\,{\rm d}t & \text{(MD)}.
	      \end{align*}
	    </p>
	    <p class="fragment">
	      Or you can directly sample the numbers/states ${\boldsymbol\Gamma}$ as you know their probability distribution beforehand,
	      \begin{align*}
	      \langle{\mathcal A}\rangle &= \int d{\boldsymbol\Gamma}\,{\mathcal P}({\boldsymbol\Gamma})\,
	      {\mathcal A}({\boldsymbol\Gamma}) & \text{(MC)}.
	      \end{align*}
	    </p>
	    <p class="fragment">
	      The system must be ergodic and you must know the
	      (relative) probability of the states for these averages
	      to be equal. Suprisingly, this is often the case for
	      molecular systems (see stat. mech. lectures).
	    </p>
	  </section>
	  <section data-background="img/Casino_de_Monte-Carlo.png">
	    <h2 class="backbox">Monte Carlo</h2>
	    <div>
	      <p class="backbox">
		Molecular dynamics has a very large number of
		states, we cannot visit them all.
	      </p>
	      <p class="backbox">
		Stanisław Marcin Ulam (re)invented the Monte Carlo
		method while working on the nuclear weapons project in
		1946. The code name came from the Monte Carlo Casino
		where Ulam's uncle gambled, and the idea came from
		considering the large number of states in Canfield
		solitaire.
	      </p>
	      <p class="backbox">
		The ergodic idea is that we can visit "enough" to get
		a good approximation of the observables 
		$\mathcal{A}$. My lectures will cover the basics of
		Monte Carlo integration, and its application to
		molecular systems.
	      </p>
	    </div>
	  </section>
	  <section>
	    <h2>Introduction</h2>
	    <ul>
	      <li> Monte Carlo (MC) is a stochastic molecular-simulation technique.</li>
	      <li class="fragment">
		Sampling can be biased towards regions of specific interest (see MC 3).
	      </li>
	      <li class="fragment">
		Not constrained by natural timescales (as in molecular
		dynamics) but time does not generally exist in MC
		simulation.
	      </li>
	      <li class="fragment">
		No (physical) dynamic foundation for the systems
		considered here.
	      </li>
	      <li class="fragment">
		Can <em>easily</em> handle fluctuating numbers of
		particles (as in the grand canonical ensemble)
		hard for molecular dynamics, but actually the
		closest ensemble to real experiments.
	      </li>
	    </ul>
	  </section>
	  <section>
	    <h2>Overview</h2>
	    <ul>
	      <li> Introduction</li>
	      <li> Practicalities</li>
	      <li> Free energy barriers</li>
	      <li> Advanced algorithms</li>
	      <li> Further reading:
		<ul>
		  <li> MP Allen and DJ Tildesley, Computer Simulation of Liquids (1987).
		  </li>
		  <li> D Frenkel and B Smit, Understanding Molecular Simulation (2001).
		  </li>
		  <li> DP Landau and K Binder, A Guide to Monte Carlo Simulations in
		    Statistical Physics (2000).
		  </li>
		</ul>
	      </li>
	    </ul>
	  </section>
	  <section>
	    <b>Monte Carlo evaluation of integrals</b>
	    \begin{equation*}
	    I = \int_a^b f(x)\,{\rm d}x
	    \end{equation*}
	    <ul style="font-size:80%">
		<li> Implicit uniform probability/sampling distribution ${\mathcal P}(x)$ along the
		    interval $[a,b]$:
		    \begin{equation*}
		    {\mathcal P}(x) =  \left\{
		    \begin{array}{ll}
		  \left(b-a\right)^{-1} & \mbox{for $a \le x \le b$} \\
		    0 & \mbox{for $x\lt a$ or $x\gt a$} \\
		    \end{array}
		\right.
		\end{equation*}
	      </li>
	      <li class="fragment">
		  If we randomly sample points $x_1$, $x_2$, $\cdots$
		  from ${\mathcal P}(x)$. The integral is related to
		  the average of the function $f$:
		  
		\begin{equation*} I = (b-a) \int_a^b {\mathcal
		P}(x)\,f(x) {\rm d}x = (b-a) \langle f(x) \rangle
		\end{equation*}</li>
	      <li class="fragment"> The average can be approximated by:
		\begin{equation*}
		I \approx \frac{(b-a)}{N} \sum_{k=1}^N f(x_k)
		\end{equation*}
	      </li>
	      <li class="fragment">
		  Ulam's initial application was to calculate the chance
		  of winning Canfield solitaire, where the tediousness
		  of combinatorial calculations could be replaced by
		  simply playing a large number of games.
	      </li>
	    </ul>
	  </section>
	  <section>
	      <h2>Convergence</h2>
	      <p>
		  <em>Monte Carlo is an extremely bad method; it should be used only when all alternative methods are worse.<br/><span style="float:right;">A. D. Sokal</span></em>
	      </p>
	  </section>
	  <section>
	      <h2>Example: Convergence</h2>
	      <video autoplay controls loop style="width:70%">
		  <source src="img/pi_calc_1.mp4" type="video/mp4"/>
	      </video>
	      <!-- <img src="img/MC1_integrate_convergence.png" width="50%" height="auto" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);"> -->
	      <p>
		  Calculating $\pi=\int^1_{-1}\int^1_{-1} \theta(1-x^2-y^2){\rm d}x\,{\rm d}y$ by randomly dropping points in a
		  square and testing if they fall in the circle (fraction should be $\pi/4$).
	      </p>
	  </section>
	  <section>
	      <h2>Example: Convergence</h2>
	      <video autoplay controls loop style="width:70%">
		  <source src="img/pi_calc_2.mp4" type="video/mp4"/>
	      </video>
	      <p>
		  Rerunning the simulation yields different scatter, but similar rate of convergence.
	      </p>
	  </section>
	  <section>
	      <img src="img/MC1_integrate_convergence.png" width="50%" height="auto" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);">
	      <p>
		  The average of $N$ normally distributed random
		  numbers, centered about $1$, with standard deviation
		  of $\sigma_f$ also gives the same sort of convergence!</p>
	  </section> 
	  <section>
	      <img src="img/MC1_integrate.png"  width="50%" height="auto" style="margin:0;">
	      <p>
		  Distribution of results as a function of $N$, the
		  number of random points used for the simple integral
		  of $sin(x)$.<br/>
		  Note the convergence and evolution of a peaked
		  distribution&hellip;</p>
	  </section>
	  <section>
	      <h2>Central limit theorem</h2>
	      <p>
		  Consider a set of $N$ independent variables $x_1$, $x_2$,...,$x_N$
		  which all follow the same probability distribution function $P(x)$,
		  with mean, $\mu$, and variance, $\sigma$.
	      </p>
	      <p class="fragment">
		  How is the mean $\bar{x}=\frac{1}{N}\sum_{k=1}^N x_k$ 
		  of these variables distributed?
	      </p>
	      <p class="fragment">
		  This is given by the <b>central limit theorem</b>, which
		  becomes increasingly accurate as $N\to\infty$
		  \begin{align*}
		  p_{\bar{x}}(\bar{x}) &\approx
		  (2\pi\sigma_{\bar{x}}^2)^{-1/2} 
		  \exp\left(-\frac{(\bar{x}-\mu)^2}{2\sigma_{\bar{x}}^2}\right)
		  \\
		  \sigma_{\bar{x}} &\approx \frac{\sigma}{\sqrt{N}}
		  \end{align*}
	      </p>
	      <p class="fragment">
		  All our results follow this so far, and it applies
		  to most processes, physics has never been so easy&hellip;
	      </p>
	  </section>
	  <section>
	      <img src="img/MC1_centrallimit1.png" width="55%" height="auto" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);">
	      <p>
		  The <b>uniform distribution</b> (black), averaged over
		  multiple ($N$) samples, tends towards a peaked gaussian
		  distribution.</p>
	  </section>
	  <section>
	      <img src="img/MC1_centrallimit2.png" width="55%" height="auto" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);">
	      <p>
		  This is also true for the <b>exponential distribution</b>
		  (black), and many others.
	      </p>
	  </section>
	  <section style="height:100%; width:100%;">
	    <b>Example:  Galton Board</b>
	    <div style="height:100%;width:100%;">
	      <iframe src="http://www.youtube.com/embed/03tx4v0i7MA" frameborder="0" allowfullscreen style="width:100%;"></iframe>
	    </div>
	  </section>
	  <section>
	      <h2>Convergence</h2>
	      <p>
		  How quickly does the average of central-limit
		  processes (like Monte Carlo) converge to the correct answer?<br/>
		  Central limit theorem:
		  \begin{align*}
		  \sigma_{\left\langle f\right\rangle} &= \frac{\sigma_f}{\sqrt{N}}
		  &
		  \sigma_{f} &= \left\langle \left(f(x)\right)^2\right\rangle - \left\langle f(x)\right\rangle^2
		  \end{align*}
	      </p>
	      <p class="fragment">
		  This "slow" $N^{-1/2}$ convergence is unavoidable! But we
		  can reduce $\sigma_f$ by a factor of $10^6$ or more
		  using importance sampling (more in a moment).</p>
	      <p class="fragment">
		  However, the dependence of the convergence with
		  sample size is independent of the dimensionality of
		  the integral, so convergence is independent of
		  dimensionality (molecular systems typically have at
		  least $d\,N$ dimensions!).
	      </p>
	  </section>
	  <section>
	    <h2>Multidimensional integrals</h2>
	    <p>
	      The position and velocity of every atom/molecule form a $3N$-dimensional phase space, so we need multidimensional MC.
	      
	      \begin{equation*}
	      I = \int_{V} f({\bf x}) {\rm d}{\bf x}
	      \end{equation*}
	      <ul>
		<li> Uniform, probability distribution ${\mathcal P}({\bf x})$ within the
		  region $V$:
		</li>
		<li> Randomly sample points ${\bf x}_1$, ${\bf x}_2$, $\cdots$ from
		  ${\mathcal P}({\bf x})$.
		</li>
		<li> The integral is related to the average of the function $f$:
		  \begin{equation*}
		  I = V \int_{V} {\mathcal P}({\bf x})\,f({\bf x})\,{\rm d}{\bf x}
		  = V \langle f({\bf x}) \rangle
		  \end{equation*}
		</li>
		<li> The average can be approximated by:
		  \begin{equation*}
		  I \approx \frac{V}{N} \sum_{k=1}^N f({\bf x}_k)
		  \end{equation*}
		</li>
	      </ul>
	    </p>
	  </section>
	  <section>
	      <b>Importance sampling</b>
	    \begin{equation*}
	    I = \int_{V} f({\bf x})\,{\rm d}{\bf x}
	    \end{equation*}			
	    <img src="img/importance.png" width="30%" height="auto" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1); float:right;">
	    <div style="width:60%; float:left">
	      <ul>
		  <li> Choose distribution ${\mathcal P}$ to make $f({\bf x})/{\mathcal
		      P}({\bf x})$ as constant as possible:
		      \begin{equation*}
		      I = \int_{V} {\mathcal P}({\bf x}) \,
		      \frac{f({\bf x})}{{\mathcal P}({\bf x})}{\rm d}{\bf x}
		      \end{equation*}
		  </li>
		  <li class="fragment"> Sample using $x$ from now non-uniform distribution ${\mathcal P}(x)$
		      \begin{equation*}
		      I \approx \frac{V}{N} \sum_k   
		      \frac{f({\bf x})}{{\mathcal P}({\bf x})}
		      \end{equation*}
		  </li>
		  <li class="fragment">
		    The variance can be significantly reduced if the
		    sharp peaks can be flattened.
		</li>	
	      </ul>
	    </div>
	    <span style="font-size:80%;">
		\begin{align*}
		f(x) &= x^{-1/3} + \frac{x}{10}
		\\
		\mathcal{P}(x) &= \frac{2}{3} x^{-1/3}
		\end{align*}
		<p style="font-size:50%">
		    Factor of $2/3$ in $\mathcal{P}$ is to normalise the distribution over $[0,1]$.
		</p>
	    </span>
	  </section>
	  <section>
	      <img src="img/MC1_importance.png" width="55%" height="auto" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);">
	    <p>
		Curves represent the distribution of results for the
		previous integral when MC using uniform (unfilled) and
		importance (filled) sampling.
	    </p>
	  </section>
	  <section>
	    <h2>Sampling from an arbitrary distribution</h2>
	    <ul>
	      <li >
		We need to sample molecular coordinates according to
		probability distributions from statistical mechanics.
	      </li>
	      <li class="fragment"> Importance sampling also requires the generation of numbers from an
		arbitrary probability distribution.
	      </li>
	      <li class="fragment"> One dimensional distributions can be sampled by sampling
		uniformly from the cumulative distribution function.
	      </li>
	      <li class="fragment"> In higher dimensions, sampling algorithms are only available for
		certain distributions (e.g., Gaussian distributions).
	      </li>
	      <li class="fragment"> The Metropolis method allows the sampling of arbitrary,
		multidimensional distributions.  This relies on constructing a Markov process.
	      </li>
	    </ul>
	  </section>
	  <section>
	    <h2>Markov process</h2>
	    A Markov process is a stochastic process where the future state of the
	    system depends only on the present state of the system and not on its
	    previous history.
	    <ul>
	      <li class="fragment"> ${\mathcal P}_\alpha^{(n)}$ probability that the state $\alpha$ is
		occupied at step $n$.
	      </li>
	      <li class="fragment"> Transition probability $W_{\alpha'\leftarrow\alpha}$ <br/>
		Probability that a system in state $\alpha'$ will transition to
		state $\alpha$.
	      </li>
	      <li class="fragment"> Markov chain:
		\begin{equation*}
		{\mathcal P}_\alpha^{(n+1)} 
		= \sum_{\alpha'} W_{\alpha\leftarrow\alpha'} {\mathcal P}_{\alpha'}^{(n)}
		\end{equation*}
	      </li>
	      <li class="fragment"> What guarantees a stationary, limiting distribution
		${\mathcal P}_\alpha$ even exists?
	      </li>
	    </ul>
	  </section>
	  <section>
	    <h2>Transition probabilities</h2>
	    <ul>
	      <li> $W_{\alpha'\leftarrow \alpha}$ is the probability that the
		system will transition to state $\alpha'$, given that it is in state
		$\alpha$.
	      </li>
	      <li> The transition matrix must satisfy:
		\begin{equation*}
		\sum_{\alpha'} W_{\alpha'\leftarrow \alpha} = 1
		\end{equation*}
	      </li>
	      <li> The choice of the transition matrix determines the properties of
		the Markov process.
	      </li>
	    </ul>
	  </section>
	  <section>
	    <h2>Stationary distribution</h2>
	    <p>
	      We need to choose the transition matrix to ensure that the stationary
	      distribution
	    </p>
	    <ul>
	      <li> exists and is unique
		\begin{equation*}
		{\mathcal P}_\alpha =  
		\sum_{\alpha'} W_{\alpha\leftarrow\alpha'} {\mathcal P}_{\alpha'}
		\end{equation*}
	      </li>
	      <li> is rapidly approached
		\begin{align*}
		\lim_{n\to\infty} {\mathcal P}_\alpha^{(n)} &= {\mathcal P}_\alpha
		\\
		{\mathcal P}_\alpha^{(n)}
		&= \sum_{\alpha_1,\alpha_2,\dots,\alpha_n}
		W_{\alpha\leftarrow\alpha_1}W_{\alpha_1\leftarrow\alpha_2}\cdots 
		W_{\alpha_{n-1}\leftarrow\alpha_n}{\mathcal P}_{\alpha_n}^{(0)}
		\\
		&= \sum_{\alpha'}
		W_{\alpha\leftarrow\alpha'}^n{\mathcal P}_{\alpha'}^{(0)}
		\end{align*}
	      </li>
	    </ul>
	  </section>
	  <section>
	    <h2>Detailed balance</h2>
	    <ul>
		<li> If the transition probability satisfies the detailed balance
		    condition, the Markov process has a unique, stationary limiting
		    distribution.
		</li>
		<li class="fragment"> Physically, the condition of detailed balance requires that the
		    probability of observing the system transition from state a $\alpha$
		    to another state $\alpha'$ is the same as observing it transition
		    from $\alpha'$ to $\alpha$.
		</li>
		<li class="fragment"> Mathematically, this is
		    \begin{equation*}
		    W_{\alpha\leftarrow \alpha'} {\mathcal P}_{\alpha'} 
		=  W_{\alpha'\leftarrow \alpha} {\mathcal P}_\alpha
		\end{equation*}
	      </li>
	      <li class="fragment"> Detailed balance is not required for the Markov
		process to possess a unique, stationary limiting
		distribution, but its a nice way to make sure.</li>
	    </ul>
	  </section>
	  <section>
	    <h2>Metropolis method</h2>
	    <ul>
	      <li> The Metropolis method provides a manner to sample from any
		probability distribution ${\mathcal P}_\alpha$, using a Markov
		process.
	      </li>
	      <li> The transition probabilities are divided into two contributions:
		\begin{equation*}
		W_{\alpha'\leftarrow \alpha} 
		= A_{\alpha'\leftarrow \alpha} g_{\alpha'\leftarrow \alpha}
		\end{equation*}
		where $g_{\alpha'\leftarrow \alpha}$ is the probability of selecting a
		move to state $\alpha'$, given the system is at state $\alpha$, and
		$A_{\alpha'\leftarrow \alpha}$ is the probability of accepting the
		proposed move.
	      </li>
	      <li> The acceptance probability is given by
		\begin{equation*}
		A_{\alpha'\leftarrow\alpha} = \left\{
		\begin{array}{ll}
		1 & \mbox{if ${\mathcal P}_{\alpha'}\gt {\mathcal P}_\alpha$} \\
		\frac{{\mathcal P}_{\alpha'}}{{\mathcal P}_\alpha} 
		& \mbox{if ${\mathcal P}_{\alpha'}\lt {\mathcal P}_\alpha$}
		\end{array}
		\right.
		\end{equation*}
	      </li>
	      <li> If $g_{\alpha'\leftarrow \alpha}=g_{\alpha\leftarrow \alpha'}$,
		then the process satisfies detailed balance.
	      </li>
	    </ul>
	  </section>
	  <section>
	    <h2>Metropolis method</h2>
	    <object type="image/svg+xml" data="img/metropolis.svg" width="60%" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);">
	      Your browser does not support SVG
	    </object>
	  </section>
	  <section>
	    <h2>Markov process: Continuous state space</h2>
	    <ul>
		<li>
		    The generalisation to continuous systems is straightforward.
		</li>
		<li> ${\mathcal P}^{(n)}(x)$ probability that the state $x$ is
		    occupied at step $n$.
		</li>
		<li> Transition probability $W(x'\leftarrow x)$ <br/>
		    Probability that a system in state $x$ will transition to state $x'$.
		</li>
		<li> Markov chain
		    \begin{equation*}
		    {\mathcal P}^{(n+1)}(x) = \int W(x\leftarrow x')\,{\mathcal P}^{(n)}(x')\,{\rm d}{\bf x}'
		    \end{equation*}
		</li>
	    </ul>
	  </section>
	  <section>
	    <h2>Random number generators</h2>
	    <em>Any one who considers arithmetical methods of producing random digits is, of course, in a state of sin.<span style="float:right;">J. von Neumann</span></em>
	    <p class="fragment">
	      von Neumann is simply trying to caution
	      you against believing that random number
	      generators (RNG) are actually random.
	    </p>
	    <p class="fragment">
	      But GPU graphics programmers really do use a state of $\sin$!
	      \begin{align*}
	      x_i  = {\rm fract}(43758.5453\sin(12.9898\,i))
	      \end{align*}
	    </p>
	    <p class="fragment">
	      When performing Monte Carlo, you must make sure your
	      randomness is sufficiently random.
	    </p>
	  </section>
	  <section>
	    <h2>Pseudo-random number generators</h2>
	    <ul>
	      <li> Typically, random deviates are
		available with a uniform distribution
		on $[0,1)$ in most programming languages.</li>
	      <li> Be suspicious of intrinsic "<tt>ran</tt>" functions
	      </li>
	      <li> Linear congruential generators are often used for these and use the recursion
		\begin{equation*}
		X_{j+1} = a X_j + b \qquad \mod m
		\end{equation*}
	      </li>
	      <li> Written ${\rm LCG}(m,a,b,X_0)$
	      </li>
	      <li> Generates integers in range $0 \le X \le m$
	      </li>
	      <li> Extremely fast but sequence repeats with period $m$ AT BEST
	      </li>
	      <li> 32-bit: $m \sim 2^{31} \sim 10^9$, whereas 64-bit: $m \sim 10^{18}$
	      </li>
	    </ul>
	  </section>
	  <section>
	    <h2>Tests for random number generators</h2>
	    <ul>
	      <li> RNGs assessed by plotting $d$-dimensional vectors
		$(X_{n+1},\dots,X_{n+d})$ and looking for &ldquo;planes&rdquo; (which are bad)
	      </li>
	      <li> 3d plots: good (left); bad $LCG(2^{31},65539,0,1)$ (right)

		<img src="img/RNGgood.png" width="30%" height="auto" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);">
		<img src="img/RNGbad.png" width="30%" height="auto" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);">

	      </li>
	      <li> In addition, simulate a finite-size system for which the
		properties are known exactly (e.g., 2D Ising model on square
		lattice)
	      </li>
	      <li>
		The gold-standard battery of tests are the <a href="https://en.wikipedia.org/wiki/Diehard_tests">Diehard tests</a>.
	      </li>
	    </ul>
	  </section>
	  <section>
	    <h2>Mersenne Twister</h2>
	    <ul>
	      <li> Based on Mersenne prime $2^{19937}-1$.
	      </li>
	      <li> Passes many tests for statistical randomness.
	      </li>
	      <li> Default random number generator for Python, MATLAB, Ruby, etc.
		  Available in standard C++.  Code is also freely available in many
		  other languages (e.g., C, FORTRAN 77, FORTRAN 90, etc.).
	      </li>
	    </ul>
	  </section>
	</section>
	<section id="MC2">
	    <section>
		<h2 class="backbox">Monte Carlo 2</h2>
		<p><em> Using lists of "truly random" random numbers
		    [provided by experiment via punch cards] was extremely slow, but von
		    Neumann developed a way to calculate pseudorandom
		    numbers using the middle-square method. Though this
		    method has been criticized as crude, von Neumann was
		    aware of this: he justified it as being faster than
		    any other method at his disposal, and also noted that
		    when it went awry it did so obviously, unlike methods
		    that could be subtly
		    incorrect.<span style="float:right;">Taken from
		    the <a href="https://en.wikipedia.org/wiki/Monte_Carlo_method">MC
		    wikipedia page</a></span></em>
		</p>
		<p class="fragment">
		    Subtle errors are easy to make. We must not violate
		    detailed balance (or do so very very carefully), and
		    must have good tests to ensure the correct ensemble is
		    simulated.
		</p>
	    </section>
	    <section>
		<h2>Overview</h2>
		<ul>
		    <li> Monte Carlo in the NVT ensemble
		    </li>
		    <li> Practicalities
		    </li>
		    <li> Ergodicity and free-energy barriers
		    </li>
		    <li> Measuring ensemble averages
		    </li>
		    <li> Monte Carlo in the isobaric-isothermal ensemble
		    </li>
		    <li> Monte Carlo in the grand canonical ensemble
		    </li>
		    <li> Example: Truncated, shifted Lennard-Jones fluid
		    </li>
		    <li> Finite-size effects
		    </li>
		</ul>
	    </section>
	    <section>
		<h2>Canonical ensemble</h2>
		<ul>
		    <li> Constant particle number, volume, and temperature; fluctuating
			energy.
		    </li>
		    <li> Distribution of energies:
			\begin{equation*}
			{\mathcal P}(E;N,V,\beta) 
			\propto \Omega(N,V,E)\,e^{ - \beta E}
			\end{equation*}
		    </li>
		    <li> Distribution of individual states:
			\begin{equation*}
			{\mathcal P}({\boldsymbol\Gamma};N,V,\beta) 
			\propto e^{ - \beta H({\boldsymbol\Gamma})}
			\end{equation*}
		    </li>
		    <li> Partition function:
			\begin{align*}
			Q(N,V,\beta) = \int {\rm d}E\,\Omega(N,V,E)\,e^{- \beta E} = \int {\rm d}{\boldsymbol\Gamma}\,e^{- \beta H({\boldsymbol\Gamma})}
			\end{align*}
		    </li>
		    <li> Helmholtz free energy
			\begin{equation*}
			F(N,V,T) = -k_B\,T\ln Q(N,V,\beta)
			\end{equation*}
		    </li>
		</ul>
	    </section>
	    <section>
		<h2>Metropolis Monte Carlo in the NVT ensemble</h2>
		<p>
		    Boltzmann distribution:
		    $
		    {\mathcal P}({\boldsymbol\Gamma}) \propto e^{-\beta\,H({\boldsymbol\Gamma})}
		    $
		    Monte Carlo integration
		    \begin{align*}
		    \langle {\mathcal A} \rangle 
		    &= \int d{\boldsymbol\Gamma}\, {\mathcal P}({\boldsymbol\Gamma}) {\mathcal A}({\boldsymbol\Gamma}) 
		    \approx \frac{1}{{\mathcal N}}
		    \sum_{k=1}^{\mathcal N}  {\mathcal A}({\boldsymbol\Gamma}_k)
		    \end{align*}
		    (provided ${\boldsymbol\Gamma}_k$ is generated from ${\mathcal P}({\boldsymbol\Gamma})$)<br/>
		</p>
		<p class="fragment">
		    Transition probability:
		    \begin{equation*}
		    W({\boldsymbol\Gamma}_n\leftarrow{\boldsymbol\Gamma}_o) 
		    = g({\boldsymbol\Gamma}_n\leftarrow{\boldsymbol\Gamma}_o) 
		    A({\boldsymbol\Gamma}_n\leftarrow{\boldsymbol\Gamma}_o) 
		    \end{equation*}
		    where
		    \begin{align*}
		    g({\boldsymbol\Gamma}_n\leftarrow{\boldsymbol\Gamma}_o) &= \mbox{random particle move}
		    \\
		    A ({\boldsymbol\Gamma}_n\leftarrow{\boldsymbol\Gamma}_o) 
		    &= \left\{
		    \begin{array}{ll}
		    1 & \mbox{if $H({\boldsymbol\Gamma}_n) \lt  H({\boldsymbol\Gamma}_o)$} \\
		    e^{-\beta(H({\boldsymbol\Gamma}')-H({\boldsymbol\Gamma}))} 
		    & \mbox{if $H({\boldsymbol\Gamma}_n) \gt  H({\boldsymbol\Gamma}_o)$}
		    \end{array}
		    \right.
		    \end{align*}
		</p>
	    </section>
	    <section>
		<h2>Particle move</h2>
		<ul>
		    <li>
			Pick particle <i>at random</i> (to obey detailed balance)
		    </li>
		    <li>
			Alter $x$, $y$, and $z$ coordinates by displacements randomly
			from the interval $-\Delta{\bf r}_{\rm max}\lt \Delta{\bf r}\lt \Delta{\bf
			r}_{\rm max}$
			<br/>
			<img src="img/mover.png" width="35%" height="auto" style="float:right;background:white;color:black; width:40%; margin-left:auto; margin-right:auto; box-shadow: 0px 0px 5px 5px rgba(255,255,255,1);">
		    </li>
		    <li> Calculate resulting change in potential energy $\Delta U$
		    </li>
		    <li> Accept move with probability ${\rm min}(1,e^{-\beta\Delta U})$
			<ul>
			    <li> If $\Delta U\lt 0$, accept.
			    </li>
			    <li> If $\Delta U\gt 0$, generate random number $R \in[0,1)$
			    </li>
			    <li> If $R\le e^{-\beta\Delta U}$, accept.
			    </li>
			    <li> If $R\gt  e^{-\beta\Delta U}$, reject and retain old configuration.
			    </li>
			</ul>
		    </li>
		</ul>
	    </section>
	    <section>
		<h2>Particle move regions</h2>
		<object type="image/svg+xml" data="img/SingleParticleMoves.svg" width="60%" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);">
		    Your browser does not support SVG
		</object>
		<p>
		    We must be careful to ensure detailed balance,
		    when moving a particle it must be able to return to
		    its initial starting point, so spheres and cubes are
		    fine, but not triangular test regions!
		</p>
	    </section>
	    <section>
		<h2>Tuning the acceptance rate</h2>
		<ul>
		    <li> Acceptance rate controlled by maximum displacement(s).
		    </li>
		    <li> Displacements result in energy change $\Delta U$.
		    </li>
		    <li> Large displacements are unlikely to be accepted.
		    </li>
		    <li> Small displacements are more likely to be accepted, but progress
			through configuration space is slow.
		    </li>
		    <li>
			Although moves are accepted, this is not be a
			sensible limit to work in.
		    </li>
		    <li>
			Trial displacements should be adjusted to give
			maximum actual displacement per unit CPU time.
		    </li>
		</ul>
	    </section>
	    <section>
		<b>Energy autocorrelation function</b>
		<img src="img/acf.png" width="50%" height="auto" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);">
		<p>
		    In this simulation, the intermediate maximum trial
		    move distance results in the fastest decay (i.e., the
		    fastest sampling of phase space) per move.</p>
	    </section>
	    <section>
		<h2>Equilibration</h2>
		<ul>
		    <li> MC simulations converge towards equilibrium (or
			&ldquo;thermalize&rdquo;) until $P(A,\tau+1) = P(A,\tau)$
		    </li>
		    <li> Starting from a non-equilibrium configuration, the deviation
			from equilibrium decreases like $\exp(-\tau/\tau_0)$, where $\tau_0$
			is the correlation &ldquo;time&rdquo;
		    </li>
		    <li> Must only retain measurements (for ensemble
			averages etc.) when $\tau\gg\tau_0$, so that
			the initial (typically unphysical) condition
			is lost.
		    </li>
		    <li>
			See the exercises and the Molecular Dynamics simulation lectures for more information.
		    </li>
		</ul>
	    </section>
	    <section>
		<h2>Diagnostics</h2>
		<ul>
		    <li> Useful diagnostics include binning, autocorrelations, hot and
			cold starts, and configurational temperature
		    </li>
		    <li> Binning: accumulate separate averages over blocks of (say)
			$1000$ MC cycles, and watch for convergence of block averages
		    </li>
		    <li> Autocorrelations: calculate correlation function $\langle
			A(\tau)A(0)\rangle \sim \exp(-\tau/\tau_0)$ for some observable $A$,
			estimate $\tau_0$, discard configurations for $t\lt 10\,\tau_0$
		    </li>
		    <li> &ldquo;Hot/cold starts&rdquo;: check that simulations starting from
			different configurations (e.g., disordered and crystalline) converge
			to the same equilibrium state
		    </li>
		</ul>
	    </section>
	    <section>
		<h2>Configurational temperature</h2>
		<ul>
		    <li> In MD, one can use the equipartition theorem to check the
			(kinetic) temperature (although this can be misleading!) 
		    </li>
		    <li> DPD potentials: MP Allen, <em>J. Phys. Chem. B</em> 110, 3823
			(2006).
		    </li>
		    <li> In MC, configurational temperature provides a sensitive test of
			whether you are sampling the correct Boltzmann distribution for the
			prescribed value of $T$
			\begin{align*}
			k_BT_{\rm conf} 
			&= -\frac{\langle\sum_{\alpha}{\bf F}_{\alpha}\cdot{\bf F}_{\alpha}\rangle}
			{\langle\sum_{\alpha}\nabla_{{\bf r}_{\alpha}}\cdot{\bf F}_{\alpha}\rangle}
			+ O(N^{-1})
			\\
			{\bf F}_\alpha &= \sum_{\alpha'\ne\alpha} {\bf F}_{\alpha\alpha'}
			= -\sum_{\alpha'\ne\alpha} 
			\nabla_{{\bf r}_{\alpha}} u({\bf r}_{\alpha}-{\bf r}_{\alpha'})
			\end{align*}
			<ul>
		    </li>
		    <li> H. H. Rugh, Phys. Rev. Lett. 78, 772 (1997)
		    </li>
		    <li> O. G. Jepps et al., Phys. Rev. E 62, 4757 (2000)
		    </li>
			</ul>
		</ul>
	    </section>
	    <section>
		<h2>Configurational temperature: Example</h2>
		<ul>
		    <li> $T_{\rm conf}$ is sensitive to programming errors

			<img src="img/confT.png" width="45%" height="auto" style="background:white;color:black; margin:20px; margin-left:25%; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);">
		    </li>
		    <li> BD Butler et al., <em>J. Chem. Phys.</em> 109, 6519 (1998).
		    </li>
		    <li> Orientations: AA Chialvo et al., <em>J. Chem. Phys.</em> 114,
			6514 (2004).
		    </li>
		</ul>
	    </section>
	    <section>
		<h2>Ergodicity</h2>
		<ul>
		    <li> &ldquo;All areas of configuration space are accessible from any
			starting point.&rdquo;
		    </li>
		    <li> Difficult to prove a priori that an MC algorithm is ergodic.
		    </li>
		    <li> Many interesting systems possess configuration spaces with
			&ldquo;traps&rdquo;.
		    </li>
		    <li> This can render many elementary MC algorithms practically
			non-ergodic (i.e. it is very difficult to access certain
			(potentially important) regions of configuration space).
		    </li>
		    <li> Common examples of apparent non-ergodicity arise from
			free-energy barriers (e.g., in first-order phase transitions).
		    </li>
		</ul>
	    </section>
	    <section>
		<h2>Ergodicity</h2>

		<object type="image/svg+xml" data="img/ergodicity.svg" width="70%" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);">
		    Your browser does not support SVG
		</object>

	    </section>
	    <section>
		<h2>Isobaric-isothermal ensemble</h2>
		<ul>
		    <li> Constant number of particles, pressure, and temperature;
			fluctuating volume and energy.
		    </li>
		    <li> Joint distribution of volume and energy:
			\begin{equation*}
			{\mathcal P}(V,\,E;\,p,\,\beta) 
			\propto \Omega(N,\,V,\,E) e^{\beta\,p\,V - \beta\,E}
			\end{equation*}
		    </li>
		    <li> Partition function:
			\begin{align*}
			\Delta(N,\,p,\,\beta) = \int {\rm d}V \int {\rm d}E \Omega(N,\,V,\,E) e^{\beta\,p\,V - \beta\,E}
			\end{align*}
		    </li>
		    <li> Gibb's free energy
			\begin{equation*}
			G(N,\,p,\,\beta) = - k_B\,T \ln \Delta(N,\,p,\,\beta)
			\end{equation*}
		    </li>
		</ul>
	    </section>
	    <section>
		<h2>MC simulations in the isobaric-isothermal ensemble</h2>
		<ul>
		    <li> Single-particle move (results in $U\to U+\Delta U$)
			\begin{align*}
			A(o\to n) = \min(1,e^{-\beta\Delta U})
			\end{align*}
		    </li>
		    <li> Volume move $V\to V+\Delta V$ ($-\Delta V_{\rm max}\le\Delta V
			\le\Delta V_{\rm max}$ results in $U\to U+\Delta U$)
			\begin{align*}
			A(o\to n) &= \min\left(1,\,p(V+\Delta V)/p(V)\right)
			\\
			\frac{p(V+\Delta V)}{p(V)}
			&= \frac{(V+\Delta V)^N}{V^N} e^{-\beta(\Delta U + p\Delta V)}
			\end{align*}
		    </li>
		    <li> One MC sweep consists of $N_{\rm sp}$ attempted single-particle
			moves, and $N_{\rm vol}$ attempted volume moves.
		    </li>
		    <li> Single-particle moves and volume moves must be attempted at
			random with a fixed probability (and not cycled) so that $A(o\to n)
			= A(n\to o)$
		    </li>
		</ul>
	    </section>
	    <section>
		<h2>Isobaric-isothermal ensemble</h2>

		<object type="image/svg+xml" data="img/npt.svg" width="80%" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);">
		    Your browser does not support SVG
		</object><br/>
		Particle positions are scaled during volume
		move, thus one configuration state maps to
		more($\Delta V&gt;0$) OR less($\Delta V&lt;0$) states. Detailed balance then requires
		the $(V+\Delta V)^N/V^N$ term to offset this.</section>
	    <section>
		<h2>Grand canonical ensemble</h2>
		<ul>
		    <li> Constant chemical potential, volume, and temperature;
			fluctuating particle number and energy.
		    </li>
		    <li> Joint distribution of particle number and energy:
			\begin{equation*}
			{\mathcal P}(N,E;\mu,\beta) 
			\propto \Omega(N,V,E) e^{\beta \mu N - \beta E}
			\end{equation*}
		    </li>
		    <li> Partition function:
			\begin{align*}
			Z_{G}(\mu,V,\beta) = \sum_N \int dE \Omega(N,V,E) e^{\beta \mu N - \beta E}
			\end{align*}
		    </li>
		    <li> Free energy: pressure
			\begin{equation*}
			pV = k_BT \ln Z_G(\mu,V,\beta) + \mbox{constant}
			\end{equation*}
		    </li>
		</ul>
	    </section>
	    <section>
		<b>MC simulations in the grand canonical ensemble</b>
		<ul style="font-size:80%;">
		    <li> Single particle move (results in $U\to U+\Delta U$) as before.</li>
		    <li> Particle insertion:  $N\to N+1$ 
			(results in $U\to U+\Delta U$)
			\begin{align*}
			A(n\leftarrow o) 
			&= \min\left(1,\frac{{\mathcal P}(N+1)}{{\mathcal P}(N)}\right)
			\\
			\frac{{\mathcal P}(N+1)}{{\mathcal P}(N)}
			&= \frac{(zV)^{N+1}e^{-\beta(U+\Delta U)}}{(N+1)!} 
			\frac{N!}{(zV)^{N}e^{-\beta U}} 
			= \frac{zV e^{-\beta \Delta U}}{N+1} 
			\end{align*}
			where $z=e^{\beta\,\mu-\ln\Lambda^3}$ (<a href="#factorization">$\Lambda^3$
			arises from the new particle's velocity
			integral in its unadded state (see Frenkel and Smit)</a>, $N+1$ is to maintain detailed
			balance on selecting the added particle for deletion).</li>
		    <li class="fragment"> Particle deletion: $N\to N-1$
			(results in $U\to U+\Delta U$)
			\begin{align*}
			A(n\leftarrow o) 
			&= \min\left(1,\frac{{\mathcal P}(N-1)}{{\mathcal P}(N)}\right)
			\\
			\frac{{\mathcal P}(N-1)}{{\mathcal P}(N)}
			&= \frac{(zV)^{N-1}e^{-\beta(U+\Delta U)}}{(N-1)!} 
			\frac{N!}{(zV)^{N}e^{-\beta U}} 
			= \frac{N e^{-\beta \Delta U}}{zV} 
			\end{align*}
		    </li>
		</ul>
	    </section>
	    <section>
		<h2>Ensemble averages</h2>
		<ul>
		    <li> Finally(!) assuming that you are sampling the equilibrium
			distribution, ensemble averages are calculated as simple unweighted
			averages over configurations
			\begin{equation*}
			\langle {\mathcal A} \rangle 
			= \frac{1}{\mathcal N} \sum_{k} {\mathcal A}({\boldsymbol\Gamma}_k)
			\end{equation*}
		    </li>
		    <li> Statistical errors can be estimated assuming that block averages
			are statistically uncorrelated
		    </li>
		</ul>
	    </section>
	    <section>
		<h2>Properties</h2>
		<ul>
		    <li> pressure 
			$\displaystyle
			p=\left\lt \rho k_BT - \frac{1}{3V}\sum_{j\lt k}{\bf r}_{jk}\cdot 
			\frac{\partial u_{jk}}{\partial {\bf r}_{jk}}\right\gt $
		    </li>
		    <li> density
			$\displaystyle
			\rho=\left\lt \frac{N}{V}\right\gt 
			$
		    </li>
		    <li> chemical potential
			$\displaystyle
			\beta\mu-\ln\Lambda^3
			=-\ln\left\lt \frac{V\exp(-\beta\Delta U_+)}{N+1}\right\gt 
			$
		    </li>
		    <li> heat capacity
			$\displaystyle
			C_V/k_B = \frac{3}{2} N + \beta^2(\langle U^2\rangle-\langle U\rangle^2) $
			<br/>
			$\displaystyle
			C_p/k_B = \frac{3}{2} N + \beta^2(\langle H^2\rangle-\langle H\rangle^2) $
		    </li>
		    <li> isothermal compressibility
			$\displaystyle
			\kappa_T = -\frac{1}{V}\left(\frac{\partial V}{\partial p}\right)_T
			= \frac{\langle V^2\rangle-\langle V\rangle^2}
			{k_BT\langle V\rangle}
			= \frac{V(\langle N^2\rangle-\langle N\rangle^2}
			{k_BT\langle N\rangle^2}
			$
		    </li>
		</ul>
	    </section>
	    <section>
		<h2>Truncated-shifted Lennard-Jones atoms</h2>

		<img src="img/LJ.png" width="45%" height="auto" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);">
		<p style="font-size:80%;">
		    \begin{align*}
		    u_{\rm LJ}(r) &= 4\varepsilon\left[\left(\frac{\sigma}{r}\right)^{12}
		    - \left(\frac{\sigma}{r}\right)^{6}\right]
		    &
		    u(r) &= \left\{
		    \begin{array}{ll}
		    u_{\rm LJ}(r) - u_{\rm LJ}(r_c) & \mbox{for $r\lt r_c$} \\
		    0 & \mbox{for $r\gt r_c$} \\
		    \end{array}
		    \right.
		    \end{align*}
		    where $r_c=2.5$ for all that follows.
		</p>
	    </section>
	    <section>
		<h2>Results: Lennard-Jones fluid</h2>
		<img src="img/LJsnapshot.png" width="40%" height="auto" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);">
		<img src="img/LJgofr.png" width="50%" height="auto" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);">
	    </section>
	    <section>
		<h2>Results: Lennard-Jones fluid</h2>
		<img src="img/LJMCdata.png" width="45%" height="auto" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);">
		<img src="img/LJSofk.png" width="45%" height="auto" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);">
		<p>
		    Observing properties over the duration of the
		    simulation can give confidence we have reached
		    equilibrium (or become stuck...).
		</p>
	    </section>
	    <section>
		<h2>Results: Lennard-Jones fluid</h2>		
		<img src="img/LJtable.png" width="55%" height="auto" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);">
		<p>
		    Regardless of the ensemble (how we specify the
		    thermodynamic state), at the same state
		    the <b>average</b> properties are nearly
		    identical. There are many fluctuations, even in the
		    specified variables, do we have information on nearby
		    states? (tune in to MC3 for more)</p>
	    </section>
	    <section>
		<h2>Ensembles</h2>
		<ul style="font-size:80%;">
		    <li> canonical ensemble (NVT)
			<ul class="fa-ul">
			    <li><i class="fa-li fa fa-check"></i> Single-phase properties (at fixed densities)
			    </li>
			    <li><i class="fa-li fa fa-times"></i> Phase transition with change in volume (e.g., freezing)
			    </li>
			    <li><i class="fa-li fa fa-times"></i> Phase transition with change in structure (e.g., solid-solid)
			    </li>
			</ul>
		    </li>
		    <li> isobaric-isothermal ensemble (NpT)
			<ul class="fa-ul">
			    <li><i class="fa-li fa fa-check"></i> Corresponds to normal experimental conditions
			    </li>
			    <li><i class="fa-li fa fa-check"></i> Easy to measure equation of state
			    </li>
			    <li><i class="fa-li fa fa-check"></i> Density histograms ${\mathcal P}(V)$ at phase coexistence
			    </li>
			    <li><i class="fa-li fa fa-times"></i> Expensive volume moves (recalculation of total energy)
			    </li>
			</ul>
		    </li>
		    <li> grand canonical ensemble ($\mu$VT)
			<ul class="fa-ul">
			    <li><i class="fa-li fa fa-check"></i> Open systems (mixtures, confined fluids, interfaces)
			    </li>
			    <li><i class="fa-li fa fa-check"></i> Vapor-liquid coexistence and critical phenomena
			    </li>
			    <li><i class="fa-li fa fa-times"></i> Dense systems (low insertion/deletion rate)
			    </li>
			</ul>
		    </li>
		</ul>
	    </section>
	    <section>
		<h2>Finite size effects</h2>
		<ul>
		    <li> The finite size of the system (box dimension $L$, number of
			particles $N$) is a limitation for all computer simulations,
			irrespective of periodic boundary conditions.
		    </li>
		    <li> Finite-size effects become apparent when the interaction
			range or the correlation length $\xi$ becomes comparable to $L$
			\begin{equation*}
			h(r) = g(r)-1 \sim \frac{e^{-r/\xi(T)}}{r}
			\end{equation*}
		    </li>
		    <li> Systems with long-range interactions (e.g., Coulomb)
		    </li>
		    <li> Vapour-liquid critical point: $\xi(T)$ diverges like
			$|T-Tc|^{-0.63}$.
		    </li>
		</ul>
	    </section>
	</section>
	<section>
	    <h2>Thanks</h2>
	    <p>
		Thanks to Dr. Leo Lue (Strathclyde) for his excellent notes which
		this course is largely based on, also thanks to his predecessor,
		Dr Philip Camp (Edinburgh), where some slides originated from.
	    </p>
	    <p>
		Thank you to you for your attention.</p>
	</section>
	<section id="MC3">
	    <section>
		<h2 class="backbox">Monte Carlo 3: Biased sampling</h2>
		<em>
		    Some samples use a biased statistical
		    design... The
		    U.S. National Center for Health
		    Statistics...  deliberately
		    oversamples from minority populations in
		    many of its nationwide surveys in order to
		    gain sufficient precision for estimates
		    within these groups. These surveys
		    require the use of sample weights to produce proper estimates
		    across all ethnic groups. Provided that
		    certain conditions are met... these samples permit accurate
		    estimation of population parameters.
		    <br/>
		    <span style="float:right;">Taken from the <a href="https://en.wikipedia.org/wiki/Sampling_bias">sampling bias wikipedia page</a></span>
		</em>
		<br/>
		<em class="fragment">
		    God may like playing with dice but, given
		    the success of hard-spheres and
		    Lennard-Jones models, He also has a penchant
		    for playing snooker using classical
		    mechanics rules.
		    <br/>
		    <span style="float:right;">Prof. N. Allan, CCP5 summer school 2016</span>
		</em>
	    </section>
	    <section>
		<h2>Introduction</h2>
		<ul>
		    <li> Histogram methods
		    </li>
		    <li> Quasi non-ergodicity
		    </li>
		    <li> Vapor-liquid transition
		    </li>
		    <li> Gibbs ensemble MC simulations
		    </li>
		    <li> Free-energy barrier in the grand-canonical ensemble
		    </li>
		    <li> Multicanonical simulations
		    </li>
		    <li> Replica exchange
		    </li>
		</ul>
	    </section>
	    <section>
		<h2>Histogram extrapolation</h2>
		<ul>
		    <li> Consider an $NVT$ simulation at $\beta_0$, where we collect an energy histogram, ${\mathcal H}(E;\beta_0)$:
			\begin{align*}
			{\mathcal P}(E;\beta_0) &= \frac{\Omega(N,V,E)}{Q(N,V,\beta_0)} e^{-\beta_0 E}
			&
			{\mathcal H}(E;\beta_0) &\propto \Omega(N,V,E) e^{-\beta_0 E}
			\end{align*}
		    </li>
		    <li> Estimate for the density of states:
			\begin{align*}
			\Omega(N,V,E) \propto {\mathcal H}(E;\beta_0) e^{\beta_0 E}
			\end{align*}
		    </li>
		    <li> Using the estimate for $\Omega(N,V,E)$, we can estimate the
			histogram at any other $\beta$
			\begin{align*}
			{\mathcal P}(E;\beta) &= \frac{\Omega(N,V,E)}{Q(N,V,\beta)} e^{-\beta E}
			\\
			{\mathcal H}(E;\beta) &\propto \Omega(N,V,E) e^{-\beta E}
			\\
			&\propto {\mathcal H}(E;\beta_0) e^{-(\beta-\beta_0) E}
			\end{align*}
		    </li>
		</ul>
	    </section>
	    <section>
		<img src="img/energy_ext_hist.png" width="55%" height="auto" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);">
		<p>
		    Example: LJ fluid  $\rho\sigma^3=0.5$.
		Solid lines are simulations, dashed lines
		    are extrapolated histograms from
		    $k_B\,T=2\varepsilon$.
		</p>
	    </section>
	    <section>
		<img src="img/energy_ext.png" width="50%" height="auto" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);">
		<p> Example: LJ fluid  $\rho\sigma^3=0.5$. The extrapolated histograms allow evaluation of properties at any/all temperatures (within sampled states, above is at $k_B\,T=2\varepsilon$).</p>
	    </section>
	    <section>
		<h2>Histogram extrapolation: Other properties</h2>
		<ul>
		    <li> Other properties (i.e., $X$) can be extrapolated by collecting the joint
			probability distribution at $\beta_0$
			\begin{align*}
			{\mathcal P}(X,E;\beta_0) 
			&= \frac{\Omega(N,V,E,X)}{Q(N,V,\beta_0)} e^{-\beta_0 E}
			\\
			{\mathcal H}(X,E;\beta_0) 
			&\propto \Omega(N,V,E,X) e^{-\beta_0 E}
			\end{align*}
		    </li>
		    <li> Estimate for the modified density of states:
			\begin{align*}
			\Omega(N,V,E,X) \propto {\mathcal H}(X,E;\beta_0) e^{\beta_0 E}
			\end{align*}
		    </li>
		    <li> Using the estimate for $\Omega(N,V,E)$, we can estimate the histogram of property $X$ at any other $\beta$
			\begin{align*}
			{\mathcal H}(X,E;\beta) &\propto \Omega(N,V,E,X) e^{-\beta E}
			\\
			&\propto {\mathcal H}(X,E;\beta_0) e^{-(\beta-\beta_0) E}
			\end{align*}
		    </li>
		</ul>
	    </section>
	    <section>
		<h2>Example: LJ fluid</h2>

		<img src="img/virial_ext.png" width="50%" height="auto" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);">
		<p>
		    Extrapolation of the virial (i.e., excess)
		    contribution to the pressure.</p>
	    </section>
	    <section>
		<b>Histogram combining</b>
		<p>
		    <ul>
			<li> Consider the case where we perform NVT simulations at several
			    temperatures $\beta_1$, $\beta_2$,..., $\beta_n$ where we collect
			    the histograms:
			    \begin{align*}
			    {\mathcal H}(X,E;\beta_k) &\propto \Omega(N,V,E,X) e^{-\beta_k E}
			    \end{align*}
			</li>
			<li> Estimate for the density of states as a weighted sum:
			    \begin{align*}
			    \Omega(N,V,E,X) \propto 
			    \sum_k \frac{w_k}{Z_k} {\mathcal H}(X,E;\beta_k) e^{\beta_k E}
			    \end{align*}
			    where $\sum_k w_k=1$ and $Z_k=\sum_{E,X} \Omega(N,V,E,X) e^{-\beta_k E}$.
			</li>
			<li> The uncertainty in ${\mathcal H}(X,E;\beta_k)$ is
			    roughly $[{\mathcal H}(X,E;\beta_k)]^{1/2}$ (MC/Poisson).
			</li>
			<li> Determining the weights $w_k$ by
			    minimizing the uncertainty of the
			    estimate of the density of states leads to the following:
			    \begin{align*}
			    \Omega(N,V,E,X)&=\frac{\sum_j {\mathcal H}(E,X,\beta_j)}{\sum_k N_k\,Z_k^{-1}\,e^{-\beta_k\,E}}
			    \end{align*}
			    where $N_k$ is the number of samples, now solve for $Z^{-1}_k$!
			</li>
		    </ul>
		</p>
	    </section>
	    <section>
		<h2>Quasi non-ergodicity</h2>
		<ul>
		    <li> An MC algorithm may be theoretically ergodic, but in some
			cases it can be very difficult (or impossible) to sample all of
			the important regions of configuration space.
		    </li>
		    <li> 
			<img src="img/configurationspace.png" width="40%" height="auto" style="background:white;color:black;box-shadow: 0px 0px 10px 10px rgba(255,255,255,1); float:right; margin:1em;">
			Even in &ldquo;simple&rdquo; systems (e.g., mono-atomic fluids) significant
			free-energy barriers can separate these important areas.
		    </li>
		    <li> A common example is the barrier between simulated
			coexisting phases at a first-order phase transition.
		    </li>
		    <li> We concentrate on the vapour-liquid phase transition.
		    </li>
		</ul>
	    </section>
	    <section>
		<h2>Vapor-liquid transition</h2>
		<ul>
		    <li> At coexistence $T_{\rm vap}=T_{\rm liq}$, 
			$p_{\rm vap}=p_{\rm liq}$, and $\mu_{\rm vap}=\mu_{\rm liq}$
		    </li>
		    <li> The density  is the order parameter
		    </li>
		</ul>
		
		<img src="img/phasediagram.png" width="40%" height="auto" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);">
		<img src="img/vle.png" width="40%" height="auto" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);">
	    </section>
	    <section>
		<h2>Vapor-liquid transition: Difficulties</h2>
		<ul>
		    <li> In a finite-size system, the interfacial free energy (positive,
			unfavourable) is significant
		    </li>
		    <li> Free energy: $F=F_{\rm int}+F_{\rm bulk}=\gamma L^2 - AL^3$

			<img src="img/layer.png" width="80%" height="auto" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);">

		    </li>
		    <li> Direct simulation will not locate the coexistence densities
			accurately
		    </li>
		</ul>
	    </section>
	    <section>
		<h2>Gibbs ensemble Monte Carlo</h2>
		<ul>
		    <li> Two simulation boxes, $I$ and $II$
		    </li>
		    <li> Total number of particles $N$, volume $V$, temperature $T$
		    </li>
		    <li> Separate single-particle moves (thermal equilibrium)
		    </li>
		    <li> Exchange volume such that $V = V_I + V_{II}$ (equates $p$)
		    </li>
		    <li> Exchange particles such that $N = N_I + N_{II}$ (equates $\mu$)
			<div style="float:right;width:40%">
			    <img src="img/vapor.png" width="30%" height="auto" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);vertical-align:middle;">
			    $\longleftrightarrow$
			    <img src="img/liquid.png" width="30%" height="auto" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);vertical-align:middle;">
			</div>
		    </li>
		    <li> Phase coexistence in a single simulation!
		    </li>
		    <li> Not good for dense phases: low probability for simultaneous
			particle insertion (in one box) and deletion (from the other)
		    </li>
		</ul>
		<ul style="font-size:80%;">
		    <li> AZ Panagiotopoulos, <i>Mol. Phys.</i> 61, 813 (1987).
		    </li>
		    <li> D Frenkel and B Smit, <em>Understanding Molecular Simulation</em> (2001).
		    </li>
		</ul>
	    </section>
	    <section>
		<h2>Gibbs ensemble Monte Carlo</h2>

		<object type="image/svg+xml" data="img/gibbs_ensemble.svg" width="80%" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);">
		    Your browser does not support SVG
		</object>
	    </section>
	    <section>
		<h2>Phase diagram of square-well fluids</h2>
		<span style="font-size:80%;">
		    <div style="display:inline-block; width:40%;">
			$\lambda=1.375$<br/>
			<img src="img/lambda1375.png" width="100%" height="auto" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);vertical-align:middle;">
		    </div>
		    <div style="display:inline-block; width:40%;">
			$\lambda=1.5$<br/>
			<img src="img/lambda150.png" width="100%" height="auto" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);vertical-align:middle;">
		    </div>
		    $
		    \displaystyle
		    u(r) = \left\{
		    \begin{array}{rl}
		    \infty & \mbox{for $0\lt  r \lt  \sigma$} \\
		    - \epsilon & \mbox{for $\sigma\lt  r \lt  \lambda\sigma$} \\
		    0 & \mbox{for $\lambda\sigma \lt  r$} \\
		    \end{array}
		    \right.
		    $
		</span><br/>
		<span style="font-size:50%;">
		    L Vega et al., <i>J. Chem. Phys.</i> 96, 2296 (1992).
		</span>
	    </section>
	    <section>
		<h2>Phase diagram of square-well fluids</h2>
		<span style="font-size:80%;">
		    <div style="display:inline-block; width:40%;">
			$\lambda=1.75$<br/>
			<img src="img/lambda175.png" width="100%" height="auto" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);vertical-align:middle;">
		    </div>
		    <div style="display:inline-block; width:40%;">
			$\lambda=2$<br/>
			<img src="img/lambda200.png" width="100%" height="auto" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);vertical-align:middle;">
		    </div>
		    $
		    \displaystyle
		    u(r) = \left\{
		    \begin{array}{rl}
		    \infty & \mbox{for $0\lt  r \lt  \sigma$} \\
		    - \epsilon & \mbox{for $\sigma\lt  r \lt  \lambda\sigma$} \\
		    0 & \mbox{for $\lambda\sigma \lt  r$} \\
		    \end{array}
		    \right.
		    $
		</span><br/>
		<span style="font-size:50%;">
		    L Vega et al., <i>J. Chem. Phys.</i> 96, 2296 (1992).
		</span>
	    </section>
	    <section>
		<h2>MC simulations in the GC ensemble</h2>
		<ul>
		    <li> Single particle move (results in $U\to U+\Delta U$)
			\begin{align*}
			A(n\leftarrow o) = \min(1,e^{-\beta\Delta U})
			\end{align*}
		    </li>
		    <li> Particle insertion:  $N\to N+1$ 
			(results in $U\to U+\Delta U$)
			\begin{align*}
			A(n\leftarrow o) 
			&= \min\left(1,\frac{{\mathcal P}(N+1)}{{\mathcal P}(N)}\right)
			\\
			\frac{{\mathcal P}(N+1)}{{\mathcal P}(N)}
			&= \frac{(zV)^{N+1}e^{-\beta(U+\Delta U)}}{(N+1)!} 
			\frac{N!}{(zV)^{N}e^{-\beta U}} 
			= \frac{zV e^{-\beta \Delta U}}{N+1} 
			\end{align*}
		    </li>
		    <li> Particle deletion: $N\to N-1$
			(results in $U\to U+\Delta U$)
			\begin{align*}
			A(n\leftarrow o) 
			&= \min\left(1,\frac{{\mathcal P}(N-1)}{{\mathcal P}(N)}\right)
			\\
			\frac{{\mathcal P}(N-1)}{{\mathcal P}(N)}
			&= \frac{(zV)^{N-1}e^{-\beta(U+\Delta U)}}{(N-1)!} 
			\frac{N!}{(zV)^{N}e^{-\beta U}} 
			= \frac{N e^{-\beta \Delta U}}{zV} 
			\end{align*}
		    </li>
		</ul>
	    </section>
	    <section>
		<h2>Free energy barrier in GC ensemble</h2>
		<ul>
		    <li> In the grand-canonical ensemble
			\begin{equation*}
			{\mathcal P}(N)
			= \frac{Q(N,V,T)}{Z_G(\mu,V,T)}e^{\beta\mu N }
			= \frac{Q(N,V,T)}{Z_G(\mu,V,T)} z^N
			\end{equation*}
		    </li>
		    <li> In a system at coexistence well below $T_c$, the particle number
			histogram $p(N)$ [or $p(\rho=N/V)$] at constant chemical potential,
			temperature, and volume is bimodal, with almost no &ldquo;overlap&rdquo;
			between the peaks
		    </li>
		</ul>
		<img src="img/densityhistogram.png" width="34%" height="auto" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);vertical-align:middle;">
		<img src="img/densityactivity.png" width="32%" height="auto" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);vertical-align:middle;">
	    </section>
	    <section>
		<h2>Multicanonical simulations</h2>
		<ul>
		    <li> Apply a biasing potential $\eta(N)$ in the Hamiltonian to cancel
			out the barrier.
			\begin{equation*}
			{\mathcal P}_{\rm bias}(N) \propto {\mathcal P}(N) e^{-\eta(N)}
			\end{equation*}
		    </li>
		    <li> Insertion/deletion acceptance probabilities are modifed
			\begin{equation*}
			\frac{{\mathcal P}_{\rm bias}(N_n)}{{\mathcal P}_{\rm bias}(N_o)}
			= \frac{{\mathcal P}(N_n)}{{\mathcal P}(N_o)}
			e^{-[\eta(N_n)-\eta(N_o)]} 
			\end{equation*}
		    </li>
		    <li> Single-particle moves are unaffected
		    </li>
		</ul>
	    </section>
	    <section>
		<h2>Multicanonical  weights</h2>
		\begin{equation*}
		{\mathcal P}_{\rm bias}(N) \propto {\mathcal P}(N) e^{-\eta(N)}
		\end{equation*}
		<ul>
		    <li> The &ldquo;ideal&rdquo; choice for $\eta(N)$ would cancel out the barrier
			completely such that the biased probability distribution is flat,
			i.e., $\eta(N) = k_BT\ln{\mathcal P}(N)$ so that ${\mathcal P}_{\rm
			bias}(N) = {\rm constant}$
		    </li>
		    <li> However, if we knew that in advance, there would be no need for a
			simulation!
		    </li>
		    <li> Fortunately, $\eta(N)$ can be determined iteratively, to give
			successively smooth biased distributions
		    </li>
		</ul>
	    </section>
	    <section>
		<h2>Determining the multicanonical  weights</h2>
		\begin{equation*}
		{\mathcal P}_{\rm bias}(N) \propto {\mathcal P}(N) e^{-\eta(N)}
		\end{equation*}
		<ul>
		    <li>[Step 1:] Choose $z$ to be near coexistence (selected from
			hysteresis region)
		    </li>
		    <li>[Step 2:] Get good statistics in ${\mathcal P}(N)$ for $N_{\rm
			min}\le N\le N_{\rm max}$
		    </li>
		    <li>[Step 3:] Generate biasing potential
			\begin{align*}
			\eta(N) &= k_BT\ln{\mathcal P}(N)
			\\
			\eta(N\lt N_{\rm min}) &= k_BT\ln{\mathcal P}(N_{\rm min})
			\\
			\eta(N\gt N_{\rm max}) &= k_BT\ln{\mathcal P}(N_{\rm max})
			\end{align*}
		    </li>
		    <li>[Step 4:] Get good statistics in ${\mathcal P}_{\rm bias}(N)$
			(now over a wider range of $N$)
		    </li>
		    <li>[Step 5:] Generate unbiased $p(N)$
			\begin{align*}
			{\mathcal P} \propto {\mathcal P}_{\rm bias}(N) e^{\eta(N)}
			\end{align*}
			return to <b>Step 3</b>
		    </li>
		</ul>
	    </section>
	    <section>
		<h2>Multicanonical simulations: LJ fluid</h2>

		<img src="img/multihist.png" width="60%" height="auto" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);vertical-align:middle;">
	    </section>
	    <section>
		<h2>GCMC: Histogram reweighting</h2>
		<ul>
		    <li> At reciprocal temperature $\beta_0$ and activity $z_0$
			\begin{align*}
			{\mathcal P}(N,E;\mu_0,V,\beta_0) 
			= \frac{\Omega(N,V,E)}{Z_G(\mu_0,V,\beta_0)} e^{\beta_0\mu_0 N - \beta_0 U}
			\end{align*}
		    </li>
		    <li> Now consider a different activity and temperature
			\begin{align*}
			{\mathcal P}(N,E;\mu_1,V,\beta_1) 
			&= \frac{\Omega(N,V,E)}{Z_G(\mu_1,V,\beta_1)} e^{\beta_1\mu_1 N-\beta_1 U}
			\\
			&= {\mathcal P}(N,E;\mu_0,V,\beta_0) 
			\left(\frac{z_1}{z_0}\right)^N e^{-(\beta_1-\beta_0)U}
			\\ & \qquad \times
			\frac{Z_{G}(\mu_1,V,\beta_1)}{Z_{G}(\mu_0,V,\beta_0)}
			\end{align*}
		    </li>
		    <li> The ratio $Z_{G}(\mu_1,V,\beta_1)/Z_{G}(\mu_0,V,\beta_0)$ is
			just a normalizing factor,
			<span style="font-size:80%">
			    \begin{align*}
			    \sum_{N} \int dE {\mathcal P}(N,E;\mu,V,\beta)
			    = \sum_{N} {\mathcal P}(N;\mu,V,\beta)
			    = \int dE {\mathcal P}(E;\mu,V,\beta)
			    = 1
			    \end{align*}
			</span>
		    </li>
		</ul>
	    </section>
	    <section>
		<h2>Determining coexistence</h2>
		<ul>
		    <li> To find coexistence, tune $\mu$ to satisfy the &ldquo;equal area&rdquo;
			rule (not equal peak height)
			<br/>
			<img src="img/densityhistogram-coex.png" width="35%" height="auto" style="background:white;color:black; margin-left:27%; width:35%; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);vertical-align:middle;">

		    </li>
		    <li> These histograms were generated from a multicanonical simulation
			with $z=0.029$.
		    </li>
		</ul>
	    </section>
	    <section>
		<h2>Density histogram for the LJ fluid</h2>

		<img src="img/densityhistogram-LJ.png" width="55%" height="auto" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);vertical-align:middle;">

	    </section>
	    <section>
		<h2>Lennard-Jones phase diagram</h2>
		
		<img src="img/LJcoex.png" width="50%" height="auto" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);vertical-align:middle;">

		\begin{align*}
		\rho_{l/g} &= \rho_c + A|T-T_c| \pm B|T-T_c|^\beta
		\\
		\beta &= 0.3265  \qquad \mbox{(universal)}
		\end{align*}

	    </section>
	    <section>
		<h2>Replica exchange</h2>
		<ul>
		    <li> &ldquo;Rough&rdquo; energy landscapes are hard to sample at low
			temperature (get stuck in local minima)
		    </li>
		    <li> High-temperature simulations can glide over barriers
			
			<img src="img/energylandscape.png" width="30%" height="auto" style="background:white;color:black; margin-left:35%; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);vertical-align:middle;">

		    </li>
		    <li> Exchange complete configurations (with energies $U_0$ and $U_1$)
			between simulations run in parallel at different reciprocal
			temperatures ($\beta_0$ and $\beta_1$, respectively)
			\begin{equation*}
			\frac{{\mathcal P}(n)}{{\mathcal P}(o)} =
			\frac{e^{-\beta_0U_1} \times e^{-\beta_1U_0}}{e^{-\beta_0U_0} \times e^{-\beta_1U_1}}
			= e^{-(\beta_0-\beta_1 )(U_1-U_0 )}
			\end{equation*}
		    </li>
		</ul>
		<span style="font-size:50%;">
		    QL Yan and JJ de Pablo, <em>J. Chem. Phys.</em> 111, 9509 (1999),
		    A Kone and DA Kofke, <em>J. Chem. Phys.</em> 122, 206101 (2005)
		</span>
	    </section>
	    <section>
		<h2>Replica exchange: Simple example</h2>
		<p>
		    <ul>
			<li>
			    <div style="float:right; width:50%;">
				<img src="img/potential.png" width="100%" height="auto" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);vertical-align:middle;">
				<p>
				    \begin{align*}
				    U(x) &= \left[\sin\frac{\pi x}{2} \sin 5\pi x\right]^2
				    \\
				    p(x,\beta) &\propto e^{-\beta U}
				    \end{align*}
				</p>
			    </div>
			    Model one-dimensional system <br/> (after Frenkel and Smit)
			</li>
			<li> Single particle in unit box (with periodic boundary
			    conditions)
			</li>
			<li> External potential, $U(x)$.
			</li>
			<li> Start, $x = 0$.
			</li>
			<li> $\Delta x = \pm0.005$
			</li>
			<li> $10^6$ MC moves
			</li>
		    </ul>
		</p>
	    </section>
	    <section>
		<h2>Replica exchange: Simple example</h2>
		<ul>
		    <li> Without parallel tempering
		    </li>
		    <li> Simulations (point); exact (lines)
		    </li>
		</ul>

		<img src="img/noreplica.png" width="50%" height="auto" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);vertical-align:middle;">

	    </section>
	    <section>
		<h2>Replica exchange: Simple example</h2>
		<ul>
		    <li> With parallel tempering
		    </li>
		    <li> Five reciprocal temperatures ($\beta^*=0$, $4$, $8$ , $12$, and
			$16$)
		    </li>
		</ul>
		
		<img src="img/replica.png" width="50%" height="auto" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);vertical-align:middle;">
	    </section>
	</section>
	<section id="statmech">
	    <section>
		<h2 class="backbox">Statistical Mechanics Recap</h2>
		</p>
	    </section>
	    <section>
		<h2>Microscopics notation recap</h2>
		<ul>
		    <li> $N$ spherical atoms of mass $m$
		    </li>
		    <li> position of atom $\alpha$ is ${\bf r}_\alpha$
		    </li>
		    <li> momentum of atom $\alpha$ is ${\bf p}_\alpha$
		    </li>
		    <li> point in phase space: 
			${\boldsymbol\Gamma}=({\bf r}_1,\dots{\bf r}_N,{\bf p}_1,\dots,{\bf p}_N)$
		    </li>
		    <li> pairwise additive potential $u(|{\bf r}_\alpha-{\bf r}_{\alpha'}|)$
		    </li>
		    <li> Hamiltonian: 
			$H({\boldsymbol\Gamma})=U({\bf r}_1,\dots{\bf r}_N)+K({\bf p}_1,\dots,{\bf p}_N)$
			\begin{align*}
			U({\bf r}_1,\dots{\bf r}_N)
			&= \frac{1}{2} \sum_{\alpha\ne\alpha'} u(|{\bf r}_\alpha-{\bf r}_{\alpha'}|)
			\\
			K({\bf p}_1,\dots,{\bf p}_N)
			&= \sum_\alpha \frac{p_\alpha^2}{2m}
			\end{align*}
		    </li>
		</ul>
	    </section>
	    <section>
		<h2>Phase space</h2>

		<object type="image/svg+xml" data="img/phasespace.svg" width="100%" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);">
		    Your browser does not support SVG
		</object>
	    </section>
	    <section>
		<h2>Thermodynamics Recap</h2>
		1st law: Energy is conserved
		\begin{align*}
		dU &= \delta Q - \delta W
		\end{align*}
		2nd law: Entropy change is non-negative: $dS\ge\frac{\delta Q}{T}$<br/>
		Combining and including pressure-volume & particle work:
		\begin{align*}
		{\rm d}U &\le T\,{\rm d}S - p{\rm d}V + \mu\,{\rm d}N
		\\
		{\rm d}S &\ge \frac{1}{T}dU + \frac{p}{T}dV - \frac{\mu}{T}{\rm d}N
		\end{align*}
		Reversible transformations: 
		\begin{align*}
		{\rm d}U &= T\,{\rm d}S - p\,{\rm d}V + \mu\,{\rm d}N
		\end{align*}
		State functions are exact differentials:
		\begin{align*}
		{\rm d}f &= \frac{\partial f}{\partial x}{\rm d}x  
		+ \frac{\partial f}{\partial y}{\rm d}y + \frac{\partial f}{\partial z}{\rm d}z
		\end{align*}
		thus: $U(S,V,N)$, $S(U,V,N)$
	    </section>
	    <section>
		<h2>Basic concepts</h2>
		<p>
		    <b>Ergodic hypothesis: </b> A system with fixed $N$, $V$, and $E$ ($U$) is equally likely to be found in any of its $\Omega(N,V,E)$ microscopic states.<br/>
		</p>
		<p class="fragment">
		    Consider two subsets of states, $\Omega_A$ and $\Omega_B$. A system is more likely to be found in set A if $\Omega_A&gt;\Omega_B$.
		</p>
		<p class="fragment">
		    Therefore $S(\Omega_A)&gt;S(\Omega_B)$ as A is more likely, thus $S$ must be a monotonically increasing function in $\Omega$.<br/>
		</p>
		<p class="fragment">
		    As states increase multiplicatively, yet entropy is additive, the relationship must be logarithmic. 
		    \begin{equation*}
		    S(N,V,E) = k_B \ln \Omega(N,V,E)
		    \end{equation*}
		    where $k_B=1.3806503\times10^{-23}$ J K$^{-1}$ is the Boltzmann
		    constant, present for historic reasons.			    
		</p>
	    </section>
	    <section>
		Consider one microscopic state, $\boldsymbol{\Gamma}$ out of the $\Omega(N,V,E)$ microscopic states. Its probability is:
		\begin{equation*}
		{\mathcal P}({\boldsymbol\Gamma}) \propto \frac{1}{\Omega(N,V,E)}
		\end{equation*}
		
		In the $N,V,E$ ensemble, we can now determine average properties if the <b>density of states</b>, $\Omega(N,V,E)$ is known.
		
		\begin{align*}
		\langle{\mathcal A}\rangle &= \int {\rm d}{\boldsymbol\Gamma}\,{\mathcal P}({\boldsymbol\Gamma})\,
		{\mathcal A}({\boldsymbol\Gamma}) = \lim_{t_{obs.}\to\infty} \frac{1}{t_{obs.}}\int_0^{t_{obs.}}\mathcal A(t)\,{\rm d}t
		\end{align*}
	    </section>
	    <section>
		<h2>Density of states</h2>
		<ul>
		    <li> The density of states $\Omega(N,V,E)$ is the number of ways the
			system can have an energy $E$.  In other words, it is the "volume"
			of phase space with an energy $E$.
		    </li>
		    <li> Volume element in phase space:
			\begin{equation*}
			d{\boldsymbol\Gamma} = \frac{1}{N!}
			\frac{d{\bf r}_1d{\bf p}_1}{h^3}\cdots \frac{d{\bf r}_N d{\bf p}_N}{h^3}
			\end{equation*}
			where $h$ is Planck's constant.
		    </li>
		    <li> Mathematically, the density of states is given by
			\begin{equation*}
			\Omega(N,V,E) = \int_{E \lt  H({\boldsymbol\Gamma})\lt E+\delta E} d{\boldsymbol\Gamma} 
			\end{equation*}
			where $\delta E\ll E$.
		    </li>
		</ul>
	    </section>
	    <section>
		<h2>Microcanonical ensemble ($N V E$)</h2>
		<b>Fundamental equation of thermodynamics:</b>
		\begin{align*}
		{\rm d}S(N,V,E) &= \frac{{\rm d}E}{T} + \frac{p}{T}{\rm d}V - \frac{\mu}{T}{\rm d}N
		\end{align*}
		From $S(N,V,E) = k_B \ln \Omega(N,V,E)$:
		\begin{align*}
		{\rm d}\ln\Omega(N,V,E) &= \beta\,{\rm d}E + \beta\,p\,{\rm d} V - \beta\,\mu\,{\rm d}N
		\end{align*}
		where $\beta=1/(k_B\,T)$.

		<br/>
		<b>
		    Once the density of states $\Omega(N,V,E)$ is known, all the thermodynamic
		    properties of the system can be determined.
		</b>
	    </section>
	    <section>
		<h2>Free energies (other ensembles)</h2>
		
		The microcanonical ensemble ($N$, $V$, $E$) is very useful
		for molecular dynamics; however, other ensembles such as
		($N$, $V$, $T$) are more accessible in experiments.
		<br/>
		
		Helmholtz free energy: $A=U-TS$
		\begin{align*}
		{\rm d}A &\le -S\,{\rm d}T - p\,{\rm d}V + \mu\,{\rm d}N
		\end{align*}
		Reversible transformations:
		\begin{align*}
		{\rm d}A &= -S\,{\rm d}T - p\,{\rm d}V + \mu\,{\rm d}N
		\end{align*}
		State functions are exact differentials, thus:
		\begin{equation*}
		A(T,V,N)
		\end{equation*}
		Lets consider the probabilities of the ($N$, $V$, $T$) ensemble...
	    </section>
	    <section>
		<h2>Canonical ensemble  ($N V T$)</h2>
		<object type="image/svg+xml" data="img/canonical_ensemble.svg" width="50%" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);">
		    Your browser does not support SVG
		</object>
		<p>
		    \begin{equation*}
		    {\mathcal P}(E) \propto \Omega(E;E_{\rm tot})
		    \qquad \qquad
		    \Omega(E;E_{\rm tot}) = \Omega(E)  \Omega_{\rm surr.} (E_{\rm tot}-E)
		    \end{equation*}

		    (Note: the $N$ and $V$ variables for the system and the surroundings are implicit)
		</p>
		
	    </section>
	    <section>
		<h2>Canonical ensemble</h2>
		<p>
		    \begin{align*} {\mathcal P}(E,E_{\rm tot})
		    &\propto \Omega(E) \Omega_{\rm
		    surr.}(E_{\rm tot}-E) \\ &\propto
		    \Omega(E) e^{\ln \Omega_{surr.}(E_{\rm
		    tot}-E)} \end{align*}
		</p>
		<p class="fragment">
		    Performing a taylor expansion of
		    $\ln\Omega_{\rm surr.}$ around $E_{\rm tot}$.
		    
		    \begin{align*}
		    {\mathcal P}(E,E_{\rm tot}) &\propto \Omega(E) e^{\ln
		    \Omega_B(E_{\rm tot}) - E
		    \frac{\partial}{\partial E_{\rm tot}}\ln
		    \Omega_B(E_{\rm tot}) + \cdots}
		    \end{align*}
		</p>
		<p class="fragment">
		    First-order is fine as $E\ll E_{tot}$. Note that $\partial \ln \Omega(E) / \partial E=\beta$. <br/>At equilibrium the surroundings and system have the same temperature thus:
		    
		    \begin{align*}
		    {\mathcal P}(E,E_{\rm tot}) &\propto \Omega(E) e^{\ln \Omega_B(E_{\rm
		    tot}) - \beta E} \end{align*}
		    \begin{align*} {\mathcal P}(E) &\propto
		    \Omega(E) e^{- \beta E} \end{align*}
		    
		    (If the surroundings are large, $E_{\rm tot}$ is unimportant and the constant term $\ln \Omega_B(E_{\rm tot})$ cancels on normalisation)
		</p>
	    </section>
	    <section>
		<h2>Canonical ensemble</h2>
		Boltzmann distribution
		\begin{equation*}
		{\mathcal P}(E) = \frac{\Omega(N, V, E)}{Q(N,V,\beta)} e^{ - \beta\,E}
		\end{equation*}
		Canonical partition function is the normalisation:
		\begin{equation*}
		Q(N,V,\beta) = \int {\rm d}E\,\Omega(N,V,E) e^{-\beta\,E}
		\end{equation*}
		Its also related to the Helmholtz free energy
		\begin{align*}
		A &= -k_B\,T \ln Q(N,V,\beta) 
		\\
		{\rm d}A &= -S\,{\rm d}T -p\,{\rm d}V + \mu\,{rm d}N
		\\
		{\rm d}\ln Q(N,\,V,\,\beta) &= - E\,{\rm d}\beta + \beta\,p\,{\rm d}V - \beta\,\mu\,{\rm d}N
		\end{align*}
		Again, all thermodynamic properties can be derived from $Q(N,\,V,\,\beta)$.
	    </section>
	    <section>
		<h2>Canonical ensemble: Partition function</h2>
		The canonical partition function can be written in terms of an
		integral over phase space coordinates:
		\begin{align*}
		Q(N,V,\beta) &= \int dE \Omega(N,V,E) e^{-\beta E}
		\\
		&= \int dE e^{-\beta E}
		\int_{E \lt  H({\boldsymbol\Gamma})\lt E+\delta E} d{\boldsymbol\Gamma} 
		\\
		&= \int d{\boldsymbol\Gamma}\, e^{-\beta H({\Gamma})}
		\\
		&= \frac{1}{N!}\int \frac{d{\bf r}_1d{\bf p}_1}{h^3}
		\cdots \frac{d{\bf r}_N d{\bf p}_N}{h^3}
		e^{-\beta H({\Gamma})}
		\end{align*}
	    </section>
	    <section id="factorization">
		<h2>Factorization of the partition function</h2>
		<ul style="font-size:80%">
		    <li> The partition function can be factorized
			\begin{align*}
			Q(N,V,T) = \frac{1}{N!}
			\int d{\bf r}_1\cdots d{\bf r}_N 
			e^{-\beta U({\bf r}_1,\dots{\bf r}_N)}
			\int \frac{d{\bf p}_1}{h^3}\cdots\frac{d{\bf p}_N}{h^3}
			e^{-\beta K({\bf p}_1,\dots,{\bf p}_N)}
			\end{align*}
		    </li>
		    <li> The integrals over the momenta can be performed exactly
			\begin{align*}
			\int\frac{{\rm d}{\bf p}_1}{h^3}\cdots\frac{{\rm d}{\bf p}_N}{h^3}
			e^{-\beta K({\bf p}_1,\dots,{\bf p}_N)}
			&= \left[\int\frac{{\rm d}{\bf p}}{h^3} e^{-\frac{\beta\,p^2}{2\,m}} \right]^{N}
			\\
			&= \left(\frac{2\,\pi\,m}{\beta h^2}\right)^{3\,N/2}
			= \Lambda^{-3\,N}
			\end{align*}
		    </li>
		    <li> The partition function is given in terms of a configurational
			integral and the de Broglie wavelength $\Lambda$
			\begin{equation*}
			Q(N,\,V,\,T) = \frac{1}{N!\Lambda^{3\,N}}
			\int {\rm d}{\bf r}_1\cdots {\rm d}{\bf r}_N 
			e^{-\beta\,U({\bf r}_1,\dots{\bf r}_N)}
			= \frac{Z(N,\,V,\,T)}{N!\Lambda^{3\,N}}
			\end{equation*}
		    </li>
		</ul>
	    </section>
	    <section>
		<h2>Average properties</h2>
		<ul>
		    <li> The probability of being at the phase point ${\boldsymbol\Gamma}$ is
			\begin{equation*}
			{\mathcal P}({\boldsymbol\Gamma})
			= \frac{e^{-\beta\,H({\boldsymbol\Gamma})}}{Q(N,\,V,\,\beta)}
			\end{equation*}
		    </li>
		    <li> The average value of a property ${\mathcal A}({\boldsymbol\Gamma})$ is
			\begin{align*}
			\langle{\mathcal A}\rangle &= \int {\rm d}{\boldsymbol\Gamma} {\mathcal P}({\boldsymbol\Gamma})
			{\mathcal A}({\boldsymbol\Gamma})
			\\
			&= \frac{1}{Q(N,\,V,\,\beta)}\int {\rm d}{\boldsymbol\Gamma} e^{-\beta H({\boldsymbol\Gamma})}
			{\mathcal A}({\boldsymbol\Gamma})
			\end{align*}
		    </li>
		    <li> If the property depends only on the position of the particles
			\begin{align*}
			\langle{\mathcal A}\rangle &= \frac{1}{Q(N,V,\beta)}
			\int d{\bf r}_1\cdots d{\bf r}_N e^{-\beta U({\bf r}_1,\dots,{\bf r}_N)}
			{\mathcal A}({\bf r}_1,\dots,{\bf r}_N)
			\end{align*}
		    </li>
		</ul>
	    </section>
	    <section>
		<h2>Grand canonical ensemble  ($\mu V T$)</h2>
		<object type="image/svg+xml" data="img/grandcanonical_ensemble.svg" width="50%" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);">
		    Your browser does not support SVG
		</object>
		\begin{align*}
		{\mathcal P}(N,E) &\propto \Omega(N,E;N_{\rm tot},E_{\rm tot})
		\\
		\Omega(N,E;N_{\rm tot},E_{\rm tot}) 
		&= \Omega(N,E) \Omega_B(N_{\rm tot}-N,E_{\rm tot}-E)
		\end{align*}
	    </section>
	    <section>
		<h2>Grand canonical ensemble</h2>
		Boltzmann distribution
		\begin{equation*}
		{\mathcal P}(E,N) = \frac{\Omega(N, V, E)}{Z_G(\mu,V,\beta)} 
		e^{\beta \mu N - \beta E}
		\end{equation*}
		Grand canonical partition function
		\begin{equation*}
		Z_G(\mu, V, \beta)
		= \sum_{N} \int {\rm d}E\,\Omega(N,V,E) e^{\beta\,\mu\,N - \beta\,E}
		\end{equation*}
		Grand potential
		\begin{align*}
		\Omega &= - pV = -k_BT \ln Z_G(\mu,V,\beta) 
		\\
		d\Omega &= - d(pV) = -S dT - pdV - N d\mu
		\\
		d\ln Z_G(\mu, V, \beta) &= - E d\beta + \beta pdV + N d\beta\mu
		\end{align*}
	    </section>
	    <section>
		<h2>Isothermal-isobaric ensemble  ($NpT$)</h2>
		Boltzmann distribution
		\begin{equation*}
		{\mathcal P}(V,E) = \frac{\Omega(N, V, E)}{\Delta(N,p,\beta)} 
		e^{\beta pV - \beta E}
		\end{equation*}
		Isothermal-isobaric partition function
		\begin{equation*}
		\Delta(N, p, \beta)
		= \int dV \int dE\, \Omega(N,V,E) e^{\beta pV - \beta E}
		\end{equation*}
		Gibbs free energy
		\begin{align*}
		G &= -k_BT \ln\Delta(N,p,\beta) 
		\\
		dG &= -S dT + Vdp + \mu dN
		\\
		d\ln\Delta(N, p, \beta) &= - E d\beta + V d \beta p + \beta\mu dN
		\end{align*}
	    </section>
	    <section>
		<h2>$n$-particle density</h2>
		<ul style="font-size:80%">
		    <li> The $n$-particle density $\rho^{(n)}({\bf r}_1,\dots,{\bf r}_n)$
			is defined as $N!/(N-n)!$ times the probability of finding $n$
			particles in the element $d{\bf r}_1\cdots d{\bf r}_n$ of coordinate
			space.
			\begin{align*}
			&\rho^{(n)}({\bf r}_1,\dots,{\bf r}_n)
			= \frac{N!}{(N-n)!} \frac{1}{Z(N,V,T)}
			\int d{\bf r}_{n+1}\cdots d{\bf r}_N\, e^{-\beta U({\bf r}_1,\dots,{\bf r}_N)}
			\\
			&\int d{\bf r}_1\cdots d{\bf r}_n\,\rho^{(n)}({\bf r}_1,\dots,{\bf r}_n)
			= \frac{N!}{(N-n)!}
			\end{align*}
		    </li>
		    <li> This normalisation means that, for a homogeneous system,
			$\rho^{(1)}({\bf r})=\rho=N/V$
			\begin{align*}
			&\int d{\bf r}\, \rho^{(1)}({\bf r}) = N 
			\qquad \longrightarrow \qquad 
			\rho V = N
			\\
			&\int d{\bf r}_1d{\bf r}_2\, \rho^{(2)}({\bf r}_1,{\bf r}_2) = N(N-1) 
			\end{align*}
		    </li>
		</ul>
	    </section>
	    <section>
		<h2>$n$-particle distribution function</h2>
		<ul style="font-size:80%">
		    <li> The $n$-particle distribution function $g^{(n)}({\bf
			r}_1,\dots,{\bf r}_n)$ is defined as
			\begin{align*}
			g^{(n)}({\bf r}_1,\dots,{\bf r}_n) 
			&=\frac{\rho^{(n)}({\bf r}_1,\dots,{\bf r}_n)}
			{\prod_{k=1}^n \rho^{(1)}({\bf r}_i)}
			\\
			g^{(n)}({\bf r}_1,\dots,{\bf r}_n) 
			&= \rho^{-n} \rho^{(n)}({\bf r}_1,\dots,{\bf r}_n)
			\qquad \mbox{for a homogeneous system}
			\end{align*}
		    </li>
		    <li> For an ideal gas (i.e., $U=0$):
			\begin{align*}
			&\rho^{(n)}({\bf r}_1,\dots,{\bf r}_n)
			= \frac{N!}{(N-n)!} 
			\frac{\int d{\bf r}_{n+1}\cdots d{\bf r}_N}{\int d{\bf r}_{1}\cdots d{\bf r}_N}
			= \frac{N!}{(N-n)!} \frac{V^{N-n}}{V^N}
			\approx \rho^n
			\\
			&g^{(n)}({\bf r}_1,\dots,{\bf r}_n) \approx 1
			\\
			&g^{(2)}({\bf r}_1,{\bf r}_2) = 1 - \frac{1}{N}
			\end{align*}
		    </li>
		</ul>
	    </section>
	    <section>
		<h2>Radial distribution function</h2>
		<ul>
		    <li> $n=2$: pair density and pair distribution function in a
			homogeneous fluid
			\begin{align*}
			g^{(2)}({\bf r}_1,{\bf r}_2) &= \rho^{-2} \rho^{(2)}({\bf r}_1,{\bf r}_2) 
			\\
			g^{(2)}({\bf 0},{\bf r}_2-{\bf r}_1) 
			&= \rho^{-2} \rho^{(2)}({\bf 0},{\bf r}_2-{\bf r}_1) 
			\end{align*}
		    </li>
		    <li> Average density of particles at r given that a tagged
			particle is at the origin is
			\begin{equation*}
			\rho^{-1} \rho^{(2)}({\bf 0},{\bf r}) 
			= \rho g^{(2)}({\bf 0},{\bf r})
			\end{equation*}
		    </li>
		    <li> The pair distribution function in a homogeneous and isotropic
			fluid is the radial distribution function $g(r)$, 
			\begin{equation*}
			g(r) = g^{(2)}({\bf r}_1,{\bf r}_2)
			\end{equation*}
			where $r = |{\bf r}_1-{\bf r}_2|$
		    </li>
		</ul>
	    </section>
	    <section>
		<h2>Radial distribution function</h2>
		<img src="img/gofr1.png" width="40%" height="auto" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);">
		<img src="img/spheres.png" width="40%" height="auto" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);">
	    </section>
	    <section>
		<h2>Radial distribution function</h2>
		<ul>
		    <li> Energy, virial, and compressibility equations:
			\begin{align*}
			U &= N \rho \int d{\bf r} u(r) g(r)
			\\
			\frac{\beta p}{\rho} 
			&= 1 - \frac{\beta\rho}{6} \int d{\bf r} w(r) g(r)
			\\
			\rho k_BT \kappa_T &= 1 + \rho \int d{\bf r} [g(r)-1]
			\end{align*}
		    </li>
		    <li> "Pair" virial 
			\begin{equation*}
			w(r) = {\bf r}\cdot \frac{\partial u}{\partial{\bf r}}
			\end{equation*}
		    </li>
		</ul>
	    </section>
	    <section>
		<h2>Structure factor</h2>
		<ul>
		    <li> Fourier component $\hat{\rho}({\bf k})$ of the instantaneous
			single-particle density
			\begin{align*}
			\rho({\bf r}) &= \sum_\alpha \delta^d({\bf r}-{\bf r}_\alpha)
			= \int \frac{d{\bf k}}{(2\pi)^d} \hat{\rho}({\bf k}) e^{-i{\bf k}\cdot{\bf r}} 
			\\
			\hat{\rho}({\bf k}) &= \int d{\bf r} \rho({\bf r}) e^{i{\bf k}\cdot{\bf r}} 
			\end{align*}
		    </li>
		    <li> Autocorrelation function is called the structure factor $S(k)$
			\begin{align*}
			\frac{1}{N}\langle\hat{\rho}({\bf k})\hat{\rho}(-{\bf k})\rangle
			&= 1 + \frac{1}{N}\left\lt 
			\sum_{\alpha\ne\alpha'} e^{-i{\bf k}\cdot({\bf r}_\alpha-{\bf r}_{\alpha'})}
			\right\gt 
			\\
			S({\bf k}) &= 1 + \rho\int d{\bf r}[g({\bf r})-1] e^{-i{\bf k}\cdot{\bf r}}
			\end{align*}
		    </li>
		    <li> $S({\bf k})$ reflects density fluctuations and dictates
			scattering
		    </li>
		</ul>
	    </section>
	    <section>
		<h2>Fourier decomposition</h2>
		Consider the function
		\begin{align*}
		f(x) &= x^2 + \frac{1}{2}\sin 6\pi x  + 0.1 \cos 28\pi x
		\end{align*}
		Fourier representation:
		\begin{align*}
		f(x) &= \sum_{n=0} A_n e^{-i k_n x}
		\end{align*}
		where $k_n = 2\pi n$.
	    </section>
	    <section>
		<h2>Fourier decomposition</h2>

		<img src="img/decomposition.png" width="auto" height="80%" style="background:white;color:black; margin:20px; box-shadow: 0px 0px 10px 10px rgba(255,255,255,1);">

	    </section>
	    <section>
		<h2>Structure factor</h2>
		<ul>
		    <li> A peak at wavevector $k$ signals density inhomogeneity on the
			lengthscale $2\pi/k$
			<br/>
			<div style="text-align:center;">
			    <div style="width:16%;display:inline-block;">
				<img src="img/homogeneous.png" width="100%" height="auto" style="margin:0;"><br/>
				<img src="img/inhomogeneous.png" width="100%" height="auto" style="margin:0;">
			    </div>
			    <div style="width:28%;;display:inline-block;">
				<img src="img/sofk.png" width="100%" height="auto" style="margin:0;">
			    </div>
			</div>
		    </li>
		    <li> $k=0$ limit is related to the isothermal compressibility
			$\kappa_T$
			\begin{equation*}
			S(0) = 1 + \rho \int d{\bf r} [g({\bf r})-1] = \rho k_BT \kappa_T
			\end{equation*}
		    </li>
		</ul>
	    </section>
	    <section>
		<h2>Further reading</h2>
		<ul>
		    <li> D Chandler, <i>Introduction to Modern Statistical Mechanics</i>
			(1987).
		    </li>
		    <li> DA McQuarrie, <i>Statistical Mechanics</i> (2000).
		    </li>
		    <li> LE Reichl, <i>A Modern Course in Statistical Physics</i>
			(2009).
		    </li>
		    <li> JP Hansen and IR McDonald, <i>Theory of Simple Liquids</i>, 3ed
		      (2006).
		    </li>
		</ul>
	    </section>
	</section>
      </div>
    </div>
    <script>
     Reveal.initialize({
       plugins : [RevealMarkdown, RevealHighlight, RevealMath, RevealNotes],
       width:960,height:700,margin:0.1, minScale:0.2, maxScale:1.5,
       backgroundTransition: 'fade',
       pdfSeparateFragments: false,
       pdfMaxPagesPerSlide:1,
       hash: true,
       math: {
	 TeX: {
	   extensions: ["autoload-all.js"]
	 },
       }
     });
    </script>
  </body>
</html>
